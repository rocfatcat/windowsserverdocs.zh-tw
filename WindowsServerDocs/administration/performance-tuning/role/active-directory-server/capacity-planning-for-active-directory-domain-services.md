---
title: Active Directory Domain Services 的容量規劃
description: AD DS 的容量規劃期間所要考慮的因素詳細討論。
ms.prod: windows-server-threshold
ms.technology: performance-tuning-guide
ms.topic: article
ms.author: v-tea; kenbrunf
author: Teresa-Motiv
ms.date: 7/3/2019
ms.openlocfilehash: 5a9e2d39d4eedd1e8fdb4bfeaf267ad4cb4c596a
ms.sourcegitcommit: af80963a1d16c0b836da31efd9c5caaaf6708133
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 07/31/2019
ms.locfileid: "67799830"
---
# <a name="capacity-planning-for-active-directory-domain-services"></a>Active Directory Domain Services 的容量規劃

本主題原本由 Ken Brumfield (Microsoft 資深頂級現場工程師) 撰寫, 並提供 Active Directory Domain Services (AD DS) 的容量規劃建議。

## <a name="goals-of-capacity-planning"></a>容量規劃的目標

容量規劃與疑難排解效能事件不同。 它們彼此密切相關, 但截然不同。 容量規劃的目標如下:  

- 適當地執行和操作環境 
- 將花費在疑難排解效能問題的時間降到最低。  
  
在容量規劃中, 在尖峰期間, 組織可能會有 40% 處理器使用率的基準目標, 以符合用戶端效能需求, 並配合升級資料中心內的硬體所需的時間。 不過, 若要收到異常效能事件的通知, 監視警示閾值可能會在5分鐘的間隔內設定為 90%。

不同之處在于, 當容量管理閾值持續超過時 (一次性事件不成問題), 新增容量 (也就是增加更多或更快的處理器) 會是解決方案, 或在多部伺服器之間調整服務解. 效能警示閾值表示目前正在進行用戶端體驗, 而且需要立即採取步驟來解決問題。

相較之下: 容量管理的目的是要防止汽車意外發生 (防禦動力、確保 brakes 正常運作等等), 而效能疑難排解則是員警、火災部門和緊急醫療專業人員所做的事意外之後。 這就是「防禦性駕駛」 Active Directory 風格。

過去幾年來, 相應增加系統的容量規劃指引已大幅改變。 下列系統架構變更對設計和調整服務的基本假設有所挑戰:

- 64位伺服器平臺  
- 虛擬化  
- 增加耗電量的注意  
- SSD 儲存體  
- 雲端案例  

此外, 此方法會從以伺服器為基礎的容量規劃練習轉移到以服務為基礎的容量規劃練習。 Active Directory Domain Services (AD DS) 是一項成熟的分散式服務, 其中許多 Microsoft 和協力廠商產品做為後端, 會成為正確規劃的最重要產品, 以確保其他應用程式執行所需的容量。

### <a name="baseline-requirements-for-capacity-planning-guidance"></a>容量規劃指引的基準需求

在本文中, 預期會有下列基準需求:

- 讀者已閱讀並熟悉[Windows Server 2012 R2 的效能微調指導方針](https://docs.microsoft.com/previous-versions//dn529133(v=vs.85))。
- Windows Server 平臺是以 x64 為基礎的架構。 但是即使您的 Active Directory 環境安裝在 Windows Server 2003 x86 上 (現在超過支援週期的結尾), 而且具有較少 1.5 GB 的目錄資訊樹狀結構 (DIT), 而且可以輕鬆地保存在記憶體中, 這項指導方針文章仍然適用。
- 容量規劃是一種持續的程式, 您應該定期回顧環境符合預期的程度。
- 當硬體成本改變時, 將會在多個硬體生命週期進行優化。 例如, 記憶體會變得較低, 每個核心的成本會降低, 或不同儲存體選項的價格變更。
- 規劃一天的尖峰忙碌期間。 建議您在30分鐘或小時的間隔中查看這項功能。 任何較大的專案可能會隱藏實際的尖峰, 而任何較少的專案可能會因「暫時性尖峰」而失真。
- 規劃企業硬體生命週期期間的成長。 這可能包括以交錯方式升級或新增硬體的策略, 或每三到五年的完整重新整理。 每個都需要「猜測」, 因為 Active Directory 上的負載會成長多少。 記錄資料 (如有收集) 將有助於進行這種評量。 
- 規劃容錯功能。 一旦估計*n*衍生之後, 請規劃包含*n* &ndash; 1、 *n* &ndash; 2、 *n* &ndash; *x*的案例。
  - 根據組織的需求新增額外的伺服器, 以確保遺失單一或多部伺服器不會超過最大尖峰容量估計值。
  - 也請考慮成長計畫和容錯計畫必須整合。 例如, 如果需要一個 DC 來支援負載, 但是估計的負載將在下一年加倍, 而且需要兩個 Dc 總計, 則沒有足夠的容量可支援容錯。 解決方案的開頭為三個 Dc。 如果預算已緊密增加, 可能也會計畫在3或6個月後新增第三個 DC。

    >[!NOTE]
    >新增 Active Directory 感知應用程式可能會對 DC 負載造成明顯的影響, 不論負載是來自應用程式伺服器或用戶端。

### <a name="three-step-process-for-the-capacity-planning-cycle"></a>容量規劃週期的三步驟程式

在 [容量規劃] 中, 先決定所需的服務品質。 例如, 核心資料中心支援較高的平行存取層級, 並且需要更一致的使用者體驗和取用應用程式, 這需要更留意到冗余, 並將系統和基礎結構的瓶頸降到最低。 相反地, 只有少數使用者的附屬位置不需要相同層級的並行或容錯。 因此, 衛星辦公室可能不需要太多精力來優化基礎硬體和基礎結構, 這可能會導致成本節約。 此處的所有建議和指引都是為了達到最佳效能, 而且可以針對需求較低的案例選擇性地放寬。

下一個問題是: 虛擬化或實體？ 從容量規劃的觀點來看, 沒有正確或不正確的答案;您只能使用一組不同的變數。 虛擬化案例分成兩個選項的其中一個:

- 「直接對應」, 每一部主機一個來賓 (其中的虛擬化只是為了抽象化伺服器的實體硬體)
- 「共用主機」

測試和生產案例表示「直接對應」案例可視為與實體主機相同。 不過, 「共用主機」引進了一些稍後更詳細說明的考慮。 「共用主機」案例表示 AD DS 也會與資源競爭, 而在執行此作業時, 會有一些懲罰和調整考慮。

記住這些考慮之後, 容量規劃週期是一個反復的三個步驟:

1. 測量現有的環境、判斷目前系統瓶頸的位置, 並取得規劃所需容量所需的環境基本概念。
1. 根據步驟1中所述的準則來判斷所需的硬體。
1. 監視並驗證所實行的基礎結構是在規格中運作。 在此步驟中收集的部分資料會成為下一個容量規劃週期的基準。

### <a name="applying-the-process"></a>套用進程

若要優化效能, 請確定已正確選取這些主要元件, 並將其調整為應用程式負載:

1. 記憶體
1. Network
1. 儲存體
1. 處理器
1. Net Logon

AD DS 的基本儲存需求和妥善撰寫之用戶端軟體的一般行為, 可讓最多隻有10000到20000使用者的環境放棄大量的成本規劃投資, 如同實體硬體, 幾乎任何現代化的伺服器類別系統會處理負載。 話雖如此, 下表摘要說明如何評估現有的環境, 以便選取正確的硬體。 後續各節會詳細分析每個元件, 以協助 AD DS 系統管理員使用基準建議和環境特定主體來評估其基礎結構。

一般:

- 以目前的資料為基礎的任何調整大小, 對目前的環境而言都是精確的。
- 針對任何估價, 預期需求會在硬體的生命週期內成長。
- 判斷今天是否為特大型, 並擴大至較大的環境, 或在生命週期增加容量。
- 針對虛擬化, 所有相同的容量規劃主體和方法都適用, 不同之處在于虛擬化的額外負荷必須新增至任何與網域相關的專案。
- 容量規劃 (例如任何嘗試預測的專案) 並不是精確的科學。 不預期會以精確和 100% 的精確度來計算。 這裡的指引是 leanest 建議;增加容量以提供額外的安全性, 並持續驗證環境是否維持在目標上。

### <a name="data-collection-summary-tables"></a>資料收集摘要資料表

#### <a name="new-environment"></a>新增環境

| Component | 估計 |
|-|-|
|儲存體/資料庫大小|每位使用者 40 KB 到 60 KB|
|RAM|資料庫大小<br />基本作業系統建議<br />協力廠商應用程式|
|Network|1 GB|
|CPU|1000每個核心的並行使用者|

#### <a name="high-level-evaluation-criteria"></a>高階評估準則

| Component | 評估準則 | 規劃考量 |
|-|-|-|
|儲存體/資料庫大小|[[儲存體限制](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))] 中的「若要啟動磁碟重組釋放的磁碟空間」一節| |
|儲存體/資料庫效能|<ul><li>"LogicalDisk ( *\<ntds 資料庫磁片\>磁碟機*) \Avg Disk sec/Read," "LogicalDisk ( *\<NTDS 資料庫\>磁片磁碟機*) \Avg Disk sec/Write," "LogicalDisk ( *\<NTDS 資料庫磁片磁碟機)\>* \Avg Disk sec/Transfer "</li><li>"LogicalDisk ( *\<ntds 資料庫磁片\>磁碟機*) \ Reads/sec," "LogicalDisk ( *\<ntds 資料庫\>磁片磁碟機*) \ Writes/sec," "LogicalDisk ( *\<ntds 資料庫磁片磁碟機\>* ) \ 傳輸/秒」</li></ul>|<ul><li>儲存體有兩個要解決的問題<ul><li>可用空間, 其大小為現今的主軸型和 SSD 型儲存體, 大部分的 AD 環境並不相關。</li> <li>可用的輸入/輸出 (IO) 作業–在許多環境中, 這通常會被忽略。 但請務必只評估沒有足夠 RAM 的環境, 將整個 NTDS 資料庫載入記憶體中。</li></ul><li>儲存體可以是複雜的主題, 而且應該包含硬體廠商的專業知識以進行適當的調整。 特別是更複雜的案例, 例如 SAN、NAS 和 iSCSI 案例。 不過, 一般而言, 每 Gb 儲存體的成本通常會直接反對者為每個 IO 的成本:<ul><li>RAID 5 的每 Gb 成本低於 Raid 1, 但 Raid 1 的每個 IO 成本較低</li><li>以主軸為基礎的硬碟每 Gb 的成本較低, 但 Ssd 的每個 IO 成本較低</li></ul><li>重新開機電腦或 Active Directory Domain Services 服務之後, 可延伸儲存引擎 (ESE) 快取是空的, 而且在快取會備妥時, 效能將會與磁片系結。</li><li>在大部分的環境中, AD 會以隨機模式對磁片進行大量讀取的 i/o, 否定快取和讀取優化策略的優點。  此外, AD 在記憶體中的快取比大部分的儲存系統快取更大。</li></ul>
|RAM|<ul><li>資料庫大小</li><li>基本作業系統建議</li><li>協力廠商應用程式</li></ul>|<ul><li>存放裝置是電腦中最慢的元件。 可以在 RAM 中保存的越多, 就越不需要移至磁片。</li><li>請確定配置足夠的 RAM 來儲存作業系統、代理程式 (防毒軟體、備份、監視)、NTDS 資料庫和一段時間的成長。</li><li>對於將 RAM 數量最大化的環境 (例如衛星位置) 或不可行 (DIT 太大), 請參考儲存體一節, 以確保儲存體的大小正確。</li></ul>|
|Network|<ul><li>「網路介面 (\*) \Bytes Received/sec」</li><li>「Network Interface (\*) \Bytes Sent/sec」|<ul><li>一般來說, 從 DC 傳送的流量超過傳送至 DC 的流量。</li><li>因為交換的乙太網路連線是全雙工的, 所以輸入和輸出網路流量需要個別調整大小。</li><li>合併 Dc 的數目會增加用來將回應傳回給每個 DC 的用戶端要求的頻寬量, 但會接近整個網站的線性。</li><li>如果移除附屬位置 Dc, 別忘了將衛星 DC 的頻寬新增至中樞 Dc, 並用來評估會有多少 WAN 流量。</li></ul>|
|CPU|<ul><li>「邏輯磁片 ( *\<NTDS 資料庫磁片\>磁碟機*) \Avg Disk sec/Read」</li><li>「進程 (lsass)\\% 處理器時間」</li></ul>|<ul><li>將儲存區排除為瓶頸之後, 請解決所需的計算能力。</li><li>雖然不是完全線性, 但在特定範圍內的所有伺服器上使用的處理器核心數目 (例如網站) 可以用來測量支援用戶端總負載所需的處理器數量。 新增在範圍內的所有系統上維護目前服務層級所需的最小值。</li><li>處理器速度的變更, 包括電源管理的相關變更, 會影響從目前環境衍生的數位。 一般來說, 無法精確地評估從 2.5 GHz 處理器到 3 GHz 處理器的情況, 將會減少所需的 Cpu 數目。</li></ul>|
|NetLogon|<ul><li>「Netlogon (\*) \Semaphore 取得」</li><li>「Netlogon (\*) \Semaphore 超時」</li><li>「Netlogon (\*) \Average 信號保存時間」</li></ul>|<ul><li>Net Logon 安全通道/MaxConcurrentAPI 只會影響具有 NTLM 驗證和 (或) PAC 驗證的環境。 在 Windows Server 2008 之前的作業系統版本中, 預設會開啟 PAC 驗證。 這是用戶端設定, 因此 Dc 會受到影響, 直到所有用戶端系統上的關閉為止。</li><li>如果未正確調整大小, 則具有大量交叉信任驗證 (包括樹系內信任) 的環境會有更大的風險。</li><li>伺服器合併會增加跨信任驗證的平行存取。</li><li>當使用者重新驗證對新叢集節點的移時, 需要進行激增, 例如叢集轉移失敗。</li><li>個別用戶端系統 (例如叢集) 可能也需要微調。</li></ul>|

## <a name="planning"></a>規劃

在很長的時間內, 社區的大小調整 AD DS 的建議是「放入與資料庫大小相同的 RAM」。 在大部分的情況下, 這項建議是大部分需要關注的環境。 但是, 使用 AD DS 的生態系統已有更大的 AD DS 環境, 因為它在1999中的推出。 雖然計算能力的增加和從 x86 架構切換到 x64 架構的速度已細微, 但對於在實體硬體上執行 AD DS 的一組較大的客戶而言, 效能的調整會變得不相關, 虛擬化的成長將微調考慮重新引入到比以往更大的觀眾。

下列指導方針可讓您瞭解如何判斷和規劃 Active Directory 為服務的需求, 而不論其是否部署在實體、虛擬/實體混合或單純虛擬化的案例中。 因此, 我們會將評估細分為四個主要元件的每一個: 儲存體、記憶體、網路和處理器。 簡言之, 為了讓 AD DS 的效能最大化, 目標是盡可能接近處理器系結。

## <a name="ram"></a>RAM

簡單來說, 可在 RAM 中快取的越多, 就越不需要移至磁片。 若要最大化伺服器的擴充性, 最小 RAM 數量應該是目前資料庫大小的總和、SYSVOL 大小總計、作業系統建議數量, 以及代理程式的廠商建議 (防毒軟體、監視、備份等等)). 應新增額外的數量, 以配合伺服器存留期的成長。 根據環境變更的資料庫成長估計值, 這將會是可主觀的環保。

對於將 RAM 數量最大化的環境 (例如衛星位置) 或不可行 (DIT 太大), 請參考儲存體一節, 以確保存放裝置的設計正確。

在調整記憶體大小的一般內容中出現的必然結果, 是分頁檔的大小。 在與其他記憶體相關的內容中, 其目標是要最小化到速度較慢的磁片。 因此, 問題應該從「分頁檔的大小應該如何？」開始。 「需要多少 RAM 才能最小化分頁？」 本章節的其餘部分將概述後者問題的答案。 這會留下大部分的討論, 將頁面檔案大小調整為一般作業系統建議的領域, 以及設定系統記憶體轉儲的需求, 這與 AD DS 的效能無關。

### <a name="evaluating"></a>評估

網域控制站 (DC) 所需的 RAM 數量, 實際上是一個複雜的練習, 原因如下:

- 當嘗試使用現有的系統來測量需要多少 RAM 時, 很可能會發生錯誤, 因為 LSASS 會在記憶體壓力條件下修剪, 並以人工方式 deflating 需求。
- 個別 DC 只需要快取其用戶端「有趣」的主觀事實。 這表示必須在只有 Exchange 伺服器的網站中的 DC 上快取的資料, 與只會驗證使用者的 DC 上需要快取的資料非常不同。
- 針對每個 DC 依個別情況評估 RAM 的人力, 在環境變更時, 會有所承受的改變。
- 建議背後的準則將有助於做出明智的決策: 
- 可以在 RAM 中快取的越多, 就越不需要移至磁片。 
- 存放裝置是電腦最慢的元件。 在以主軸為基礎和 SSD 儲存體媒體上的資料存取權的順序是 1, 000, 1,000 倍比存取 RAM 中的資料慢。

因此, 為了達到最大的伺服器擴充性, 最小 RAM 數量是目前資料庫大小的總和、SYSVOL 大小總計、作業系統建議數量, 以及代理程式的廠商建議 (防毒軟體、監視、備份、以此類推)。 新增額外的數量以配合伺服器存留期的成長。 根據資料庫成長的估計值, 這將會是可主觀的環保。 不過, 對於具有一小組一般使用者的衛星位置, 這些需求可能會很寬鬆, 因為這些網站不需要快取, 就能為大部分的要求提供服務。

對於將 RAM 數量最大化的環境 (例如衛星位置) 或不可行 (DIT 太大), 請參考儲存體一節, 以確保儲存體的大小正確。

> [!NOTE]
> 調整記憶體大小的必然結果會調整分頁檔的大小。 因為目標是要最小化到速度較慢的磁片, 所以問題來自于「應該如何重設分頁檔案大小？」 「需要多少 RAM 才能最小化分頁？」 本章節的其餘部分將概述後者問題的答案。 這會留下大部分的討論, 將頁面檔案大小調整為一般作業系統建議的領域, 以及設定系統記憶體轉儲的需求, 這與 AD DS 的效能無關。

### <a name="virtualization-considerations-for-ram"></a>RAM 的虛擬化考慮

避免在主機上認可記憶體。 優化 RAM 數量的基本目標是將花費在磁片上的時間降到最低。 在虛擬化案例中, 記憶體過度認可的概念存在, 而將更多的 RAM 配置給來賓, 然後存在於實體機器上。 這本身並不是問題。 當所有來賓主動使用的總記憶體超過主機上的 RAM 數量, 且基礎主機開始分頁時, 就會發生問題。 當網域控制站進入 ntds.dit 以取得資料, 或網域控制站移至分頁檔以取得資料, 或主機即將進入磁片以取得來賓認為在 RAM 中的資料時, 效能就會變成磁片界限。

### <a name="calculation-summary-example"></a>計算摘要範例

|Component|估計的記憶體 (範例)|
|-|-|
|基本作業系統建議的 RAM (Windows Server 2008)|2 GB|
|LSASS 內部工作|200 MB|
|監視代理程式|100 MB|
|防毒|100 MB|
|資料庫 (通用類別目錄)|您確定??? 8.5 GB|
|要執行備份的緩衝緩衝, 系統管理員登入而不受影響|1 GB|
|總計|12 GB|

**使用16 GB**

經過一段時間之後, 就可以將更多資料新增至資料庫, 而伺服器可能會在生產環境中有3到5年。 根據預估的 33% 成長, 16 GB 會是要放入實體伺服器的合理 RAM 數量。 在虛擬機器中, 您可以輕鬆地修改哪些設定, 而且可以將 RAM 新增至 VM, 從 12 GB 開始, 而計畫在未來監視和升級是合理的。

## <a name="network"></a>Network

### <a name="evaluating"></a>評估
這一節的目的不在於評估有關複寫流量的需求, 這著重于往返 WAN 的流量, 並已在[Active Directory 複寫流量](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10))中全面涵蓋, 而不是評估總頻寬和網路需要的容量, 包括用戶端查詢、群組原則應用程式等等。 針對現有的環境, 可以使用效能計數器「Network Interface (\*) \Bytes Received/sec」和「network interface (\*) \Bytes Sent/sec」進行收集。 網路介面計數器的取樣間隔 (15、30或60分鐘)。 任何較少的專案通常會太大, 以致于良好的測量。任何較大的東西都會非常順暢地進行每日查看。

> [!NOTE]
> 一般來說, DC 上的大部分網路流量都會在 DC 回應用戶端查詢時輸出。 這就是將焦點放在輸出流量的原因, 不過建議您也針對輸入流量評估每一個環境。 相同的方法可用來處理和檢查輸入網路流量需求。 如需詳細資訊, 請參閱知識庫[文章 929851:在 Windows Vista 和 Windows Server 2008](http://support.microsoft.com/kb/929851)中, tcp/ip 的預設動態通訊埠範圍已變更。

### <a name="bandwidth-needs"></a>頻寬需求

規劃網路擴充性涵蓋兩個不同的類別: 流量的數量, 以及來自網路流量的 CPU 負載。 相較于本文中的某些其他主題, 上述每個案例都是簡單明瞭的。

在評估必須支援多少流量時, 針對網路流量的 AD DS, 有兩個唯一類別的容量規劃。 第一種是在網域控制站之間進行的複寫流量, 會在參考[Active Directory 複寫流量](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10))中全面涵蓋, 而且仍與 AD DS 的目前版本有關。 第二個是網站間用戶端到伺服器的流量。 針對進行規劃的其中一個較簡單案例, 主要會從用戶端收到相對於傳送回用戶端之大量資料的小型要求。 100 MB 通常適用于網站中每個伺服器最多5000位使用者的環境。 針對超過5000使用者的任何專案, 建議使用 1 GB 的網路介面卡和接收端調整 (RSS) 支援。 若要驗證這種情況, 特別是在伺服器匯總案例中, 請查看網站中\*所有 dc 的網路介面 () \ 位元組/秒、將它們加在一起, 然後再除以目標網域控制站的數目, 以確保是足夠的容量。 若要這麼做, 最簡單的方法是使用 Windows 可靠性和效能監視器 (之前稱為 Perfmon) 中的「堆疊區域」視圖, 確定所有計數器都已調整為相同。

請考慮下列範例 (也就是驗證一般規則是否適用于特定環境的一種真正、複雜的方式)。 會進行下列假設:

- 其目標是要盡可能減少伺服器的使用量。 在理想的情況下, 一部伺服器將會執行負載, 並部署額外的伺服器來進行冗余 (*N* + 1 案例)。 
- 在此案例中, 目前的網路介面卡僅支援 100 MB, 並在切換的環境中。  
  在 N 案例中, 目標網路頻寬使用率上限為 60% (DC 遺失)。
- 每部伺服器都有大約10000個用戶端連線到它。

從圖表中的資料取得的知識 (網路介面 (\*) \Bytes Sent/sec):

1. 工作日會在5:30 開始向上移動, 並在下午7:00 股。
1. 尖峰時間最忙碌的期間是從上午8:00 到 8:15 AM, 在最忙碌的 DC 上, 傳送的位元組數/秒大於25。  
   > [!NOTE]
   > 所有效能資料都是歷程記錄。 所以8:15 的尖峰資料點表示從8:00 到8:15 的負載。
1. 4:00 AM 之前會有尖峰, 在最忙碌的 DC 上有20個以上的 Bytes sent/sec, 這可能表示從不同的時區或背景基礎結構活動 (例如備份) 載入。 因為上午8:00 的尖峰超過此活動, 所以不相關。
1. 網站中有五個網域控制站。
1. 負載上限大約為每 DC 5.5 MB/秒, 代表 100 MB 連接的 44%。 使用此資料, 可以預估在 8:00 AM 和 8:15 AM 之間所需的總頻寬是 28 MB/s。
   >[!NOTE]
   >請小心, 網路介面傳送/接收計數器是以位元組為單位, 而網路頻寬則以位測量。 100 MB &divide; 8 = 12.5 MB, 1 GB &divide; 8 = 128 MB。
  
結論

1. 這個目前的環境確實符合 60% 目標使用率的 N + 1 層級的容錯。 讓一個系統離線會將每個伺服器的頻寬從大約 5.5 MB/秒 (44%) 轉移約 7 MB/秒 (56%)。
1. 根據上述將合併到一部伺服器的目標, 這兩者都超過目標使用率上限, 理論上可能會使用 100 MB 連接。
1. 若使用 1 GB 連線, 這會代表總容量的 22%。
1. 在*N* + 1 案例的正常作業條件下, 用戶端負載的平均分佈大約為每個伺服器 14 MB/秒, 或總容量的 11%。
1. 為了確保在無法使用 DC 時, 容量已足夠, 每部伺服器的正常作業目標大約是每個伺服器 30% 的網路使用率或 38 MB/秒。 容錯移轉目標會是每部伺服器 60% 的網路使用率或每秒 72 MB。  
  
簡單來說, 系統的最後部署必須具有 1 GB 的網路介面卡, 並連接到支援所說負載的網路基礎結構。 還有一點要注意的是, 假設產生的網路流量量, 來自網路通訊的 CPU 負載可能會有顯著的影響, 並限制 AD DS 的最大擴充性。 這個相同的程式可以用來估計 DC 的輸入通訊量。 但是, 如果輸出流量相對於輸入流量的 predominance, 這就是大部分環境的學術練習。 確保 RSS 的硬體支援在每個伺服器的使用者人數超過5000的環境中是很重要的。 對於具有高網路流量的案例, 中斷負載的平衡可能是瓶頸。 這可由處理器 (\*)\%停機時間, 在 cpu 之間平均分散。 啟用 RSS 的 Nic 可以減輕這項限制並增加擴充性。

> [!NOTE]
> 您可以使用類似的方法來預估合併資料中心時所需的額外容量, 或淘汰衛星位置中的網域控制站。 只要將輸出和輸入流量收集到用戶端, 就會成為 WAN 連結上現在會出現的流量量。  
>  
> 在某些情況下, 您可能會遇到比預期更多的流量, 因為流量的速度較慢, 例如當憑證檢查無法符合 WAN 上的積極超時。 基於這個理由, WAN 大小和使用率應該是反復進行的進程。

### <a name="virtualization-considerations-for-network-bandwidth"></a>網路頻寬的虛擬化考慮

為實體伺服器提供建議很簡單:1 GB 適用于支援大於5000使用者的伺服器。 一旦有多個來賓開始共用基礎虛擬交換器基礎結構, 就必須額外注意, 以確保主機具有足夠的網路頻寬來支援系統上的所有來賓, 因此需要額外的嚴謹度。 這不只是確保網路基礎結構進入主機電腦的延伸。 無論網路是否包含以虛擬機器來賓身分在主機上執行的網域控制站, 以及網路流量是否已透過虛擬交換器連線, 或是否直接連線至實體交換器, 都是如此。 虛擬交換器只是上行連結需要支援所傳輸之資料量的一個元件。 因此, 連結至交換器的實體主機實體網路介面卡應該能夠支援 DC 負載, 再加上所有其他共用虛擬交換器且連線至實體網路介面卡的來賓。

### <a name="calculation-summary-example"></a>計算摘要範例

|系統|尖峰頻寬|
|-|-|
DC 1|6.5 MB/秒|
DC 2|6.25 MB/秒|
|DC 3|6.25 MB/秒|
|DC 4|5.75 MB/秒|
|DC 5|4.75 MB/秒|
|總計|28.5 MB/秒|

**使用72 MB/秒** (28.5 MB/s 除以 40%)

|目標系統計數|總頻寬 (如上)|
|-|-|
|2|28.5 MB/秒|
|產生的一般行為|28.5 &divide; 2 = 14.25 MB/秒|

一如往常, 一段時間之後, 就可以讓用戶端負載增加, 而且應該盡可能地規劃此成長。 建議的規劃數量會允許預估在 50% 的網路流量成長。

## <a name="storage"></a>儲存體

規劃儲存體構成兩個元件:

- 容量或儲存體大小
- 效能

在規劃容量時, 會花費大量的時間和檔, 讓效能經常被忽略。 使用目前的硬體成本時, 大部分的環境都不夠大, 因為這其中一項實際上是個問題, 而建議「放入與資料庫大小相同的 RAM」通常會涵蓋其餘部分, 但可能會大材小用較大的附屬位置。環境.

### <a name="sizing"></a>調整大小

#### <a name="evaluating-for-storage"></a>評估儲存體

相較于13年前, 導入 Active Directory 時, 4 GB 和 9 GB 磁片磁碟機的時間是最常見的磁片磁碟機大小, 而針對所有但最大的環境而言, Active Directory 的調整大小甚至不是一個考慮。 使用 180 GB 範圍內的最小可用硬碟大小, 整個作業系統、SYSVOL 和 ntds.dit 可以輕鬆地放在一個磁片磁碟機上。 因此, 建議您在此區域中取代大量投資。

考慮的唯一建議是確保 110% 的 ntds.dit 大小可供使用, 以啟用磁碟重組。 此外, 您也應該進行硬體生命週期的成長。

第一個和最重要的考慮是評估 ntds.dit 和 SYSVOL 的大小。 這些測量會導致固定磁片和 RAM 配置的大小調整。 由於這些元件的 (相對) 低成本, 因此數學運算不需要嚴格且精確。 有關如何針對現有和新環境評估這項操作的內容, 可以在[資料儲存](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc961771(v=technet.10))系列文章中找到。 具體而言, 請參閱下列文章:

- **針對現有環境&ndash;**  , 請參閱[儲存空間限制](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))一文中的「若要啟動磁碟重組釋放的磁碟空間記錄」一節。
- **針對新的&ndash;環境**, 文章標題為[Active Directory 使用者和組織單位的成長估計](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-2000-server/cc961779(v=technet.10))。

  > [!NOTE]
  > 本文是以 Windows 2000 中 Active Directory 發行時所做的資料大小估計為基礎。 使用物件大小, 反映您環境中實際的物件大小。

當您使用多個網域來審查現有的環境時, 資料庫大小可能會有變化。 若為 true, 請使用最小的通用類別目錄 (GC) 和非 GC 大小。  

資料庫大小可能會因作業系統版本而異。 執行舊版作業系統 (例如 Windows Server 2003) 的 Dc, 其資料庫大小比執行較新作業系統 (例如 Windows Server 2008 R2) 的 DC 小, 特別是當啟用這類 Active Directory 回收站或認證漫遊的功能時。

> [!NOTE]  
  >
>- 針對新的環境, 請注意, Active Directory 使用者和組織單位的成長估計值估計值表示100000個使用者 (在相同網域中) 耗用大約 450 MB 的空間。 請注意, 所填入的屬性可能會對總金額造成很大的影響。 協力廠商和 Microsoft 產品 (包括 Microsoft Exchange Server 和 Lync) 的屬性將會填入許多物件。 最好是根據環境中的產品群組進行評估, 但針對所有但最大的環境, 將數學運算和測試的精確估計值的做法, 可能不是真的值得大量的時間和努力。
>- 請確定 110% 的 ntds.dit 大小可作為可用空間, 以啟用離線重組, 並規劃在三到五年的硬體生命週期內成長。 由於儲存體的成本有多低, 因此在 300% 時估計儲存體的大小, 因為存放裝置配置可以安全地容納成長, 而且可能需要離線重組。

#### <a name="virtualization-considerations-for-storage"></a>儲存體的虛擬化考慮

在單一磁片區上配置多個虛擬硬碟 (VHD) 檔案的案例中, 請使用大小至少為 210% (100% 的 DIT + 110% 可用空間) 的固定大小磁片, 以確保有足夠的空間保留。  

#### <a name="calculation-summary-example"></a>計算摘要範例

|從評估階段收集的資料| |
|-|-|
|Ntds.dit 大小|35 GB|
|允許離線磁碟重組的修飾詞|2.1|
|所需的儲存體總計|73.5 GB|

> [!NOTE]
> 除了 SYSVOL、作業系統、分頁檔案、暫存檔案、本機快取資料 (例如安裝程式檔案) 和應用程式所需的儲存體之外, 還需要此儲存空間。

### <a name="storage-performance"></a>儲存體效能

#### <a name="evaluating-performance-of-storage"></a>評估儲存體的效能

做為任何電腦中最慢的元件, 存放裝置可能會對用戶端體驗造成最大的負面影響。 對於 RAM 大小建議不足的環境而言, 針對效能忽略規劃儲存體的結果可能會造成嚴重的影響。  此外, 複雜性和各種儲存體技術會進一步增加失敗的風險, 因為在不同的實體磁片上, 將作業系統、記錄和資料庫等長期最佳做法的相關性限制在其有用的案例中。  這是因為長期的最佳作法是以「磁片」是專用的主軸, 而這種允許的 i/o 隔離。  這個 true 的假設已不再與引進的相關:

- 新的儲存體類型和虛擬化和共用儲存案例
- 存放區域網路 (SAN) 上的共用磁軸
- SAN 或網路連接儲存裝置上的 VHD 檔案
- 固態硬碟
- 階層式儲存體架構 (也就是 SSD 儲存層快取較大的磁鍼型存放裝置)

簡言之, 所有儲存體效能的最終目標 (不論基礎儲存體架構和設計為何) 都是確保每秒所需的輸入/輸出作業 (IOPS) 數量可供使用, 而且這些 IOPS 會在可接受的時間範圍內發生. 本節說明如何評估基礎儲存體的 AD DS 需求, 以確保正確地設計儲存體解決方案。  由於現今儲存技術的變化, 最好與存放裝置廠商合作, 以確保有足夠的 IOPS。  針對具有本機連接儲存區的案例, 請參考附錄 C, 以瞭解如何設計傳統本機儲存案例的基本概念。  此主體通常適用于更複雜的儲存層, 也會協助提供支援後端儲存體解決方案的廠商。

- 提供廣泛的儲存體選項, 建議您與硬體支援小組或廠商的專業知識接洽, 以確保特定解決方案符合 AD DS 的需求。 以下是將提供給存放裝置專家的資訊。

對於資料庫太大而無法保留在 RAM 中的環境, 請使用效能計數器來判斷需要支援多少 i/o:

- LogicalDisk (\*) \Avg Disk sec/Read (例如, 如果 ntds.dit 儲存在 D:/磁片磁碟機, 完整路徑會是 LogicalDisk (D:) \Avg Disk sec/Read)
- LogicalDisk (\*) \Avg Disk sec/Write
- LogicalDisk (\*) \Avg Disk sec/Transfer
- LogicalDisk (\*) \ Reads/sec
- LogicalDisk (\*) \ Writes/sec
- LogicalDisk (\*) \ 傳輸/秒

這些應該以15/30/60 分鐘的間隔進行取樣, 以對目前環境的需求進行基準測試。

#### <a name="evaluating-the-results"></a>評估結果

> [!NOTE]
> 重點在於資料庫的讀取, 因為這通常是最嚴苛的元件, 所以可以將相同的邏輯套用至寫入記錄檔, 方法是替代 LogicalDisk ( *\<NTDS log\>* ) \Avg Disk sec/Write 和 LogicalDisk (*NTDS記錄\>檔) \ 寫入/秒): \<*
>  
> - LogicalDisk ( *\< NTDS\>* ) \Avg Disk sec/Read 指出目前的儲存體是否有適當的大小。  如果結果大致等於磁片類型的磁片存取時間, 則 LogicalDisk ( *\<NTDS\>* ) \ 讀取/秒是有效的量值。  查看後端存放裝置的製造商規格, 但 LogicalDisk ( *\<NTDS\>* ) \Avg Disk sec/Read 的良好範圍大致如下:
>   - 7200–9到12.5 毫秒 (毫秒)
>   - 10000–6到10毫秒
>   - 15000–4到6毫秒  
>   - SSD –1到3毫秒  
>   - >[!NOTE]
>     >有個建議存在, 表示儲存體效能會在15ms 到20毫秒時降級 (視來源而定)。  上述值和其他指引的差異在於上述值是正常作業範圍。  其他建議是疑難排解指引, 以識別用戶端體驗何時會大幅降低並變得很明顯。  如需更深入的說明, 請參閱附錄 C。
> - LogicalDisk ( *\< NTDS\>* ) \ Reads/sec 是正在執行的 i/o 數量。
>   - 如果 LogicalDisk ( *\<ntds\>* ) \Avg Disk sec/Read 是在後端儲存體的最佳範圍內, 則可以直接使用 LogicalDisk ( *\<ntds\>* ) \ Reads/sec 來調整儲存體的大小。
>   - 如果 LogicalDisk ( *\<NTDS\>* ) \Avg Disk sec/Read 不在後端儲存體的最佳範圍內, 則會根據下列公式來需要額外的 i/o:
>     > (LogicalDisk ( *\<ntds\>* ) \Avg Disk sec/read) &divide; (實體媒體磁片存取時間) &times; (LogicalDisk ( *\<NTDS\>* ) \Avg Disk sec/read)

考量：

- 請注意, 如果伺服器設定為使用最高的 RAM 數量, 則這些值不會因規劃目的而變得不正確。  它們會在較高的位置錯誤地出現, 而且仍然可以做為最糟的案例。
- 明確地新增/優化 RAM 將會降低讀取 i/o (LogicalDisk ( *\<NTDS\>* ) \ Reads/Sec 的數量。 這表示儲存體解決方案可能不需要與一開始計算的健全。  可惜的是, 比這個一般語句更具體的專案, 是取決於用戶端負載的環保, 而且無法提供一般指引。  最佳選項是在優化 RAM 之後調整儲存體大小。

#### <a name="virtualization-considerations-for-performance"></a>效能的虛擬化考慮

與上述所有虛擬化討論類似, 這裡的關鍵在於確保基礎共用基礎結構可以支援 DC 負載, 再加上使用基礎共用媒體和其所有路徑的其他資源。 無論實體網域控制站是否在 SAN、NAS 或 iSCSI 基礎結構上與其他伺服器或應用程式共用相同的基礎媒體, 都是如此, 不論其是否為使用 [傳遞存取] 的 SAN、NAS 或 iSCSI 基礎結構, 以及共用基礎媒體, 或如果來賓使用的 VHD 檔案位於本機共用媒體或 SAN、NAS 或 iSCSI 基礎結構上。 規劃的練習是要確保基礎媒體能夠支援所有取用者的總負載。

此外, 從來賓的觀點來看, 因為必須進行其他程式碼路徑的執行, 所以需要經過主機才能存取任何儲存體, 會有效能上的影響。 不驚訝的是, 存放裝置效能測試指出, 虛擬化會影響主機系統處理器使用率主觀的輸送量 (請參閱附錄 A:CPU 大小調整準則), 這顯然會受到來賓要求的主機資源所影響。 這對於虛擬化案例中有關處理需求的虛擬化考慮有貢獻 (請參閱[虛擬化考慮以進行處理](#virtualization-considerations-for-processing))。

更複雜的是, 有各種不同的儲存體選項可供使用, 它們都有不同的效能影響。 以安全的方式從實體遷移至虛擬時, 請使用乘數 1.10, 針對 Hyper-v 上的虛擬化來賓 (例如, 傳遞儲存體、SCSI 介面卡或 IDE) 調整不同的儲存選項。 在不同的儲存案例之間傳輸時, 需要進行的調整與存放裝置為本機、SAN、NAS 或 iSCSI 無關。

#### <a name="calculation-summary-example"></a>計算摘要範例

判斷狀況良好的系統在正常作業條件下所需的 i/o 數量:

- LogicalDisk ( *\<NTDS 資料庫磁片\>磁碟機*) \ 每秒在尖峰期間15分鐘期間內傳輸 
- 若要判斷儲存體所需的 i/o 數量, 以超出基礎儲存體的容量:
  >*所需的 IOPS* = (LogicalDisk ( *\<NTDS\>資料庫磁片磁碟機*) \Avg Disk &divide; sec/read  *\<Target Avg Disk\>sec/read*) &times; LogicalDisk ( *\<NTDS 資料庫磁片\>磁碟機*) \ 讀取/秒

|計數器|值|
|-|-|
|實際 LogicalDisk ( *\<NTDS 資料庫磁片\>磁碟機*) \Avg Disk sec/Transfer|02秒 (20 毫秒)|
|目標 LogicalDisk ( *\<NTDS 資料庫磁片\>磁碟機*) \Avg Disk sec/Transfer|01秒|
|可用 IO 中的變更乘數|0.02 &divide; 0.01 = 2|  
  
|值名稱|值|
|-|-|
|LogicalDisk ( *\<NTDS 資料庫磁片\>磁碟機*) \ 傳輸/秒|400|
|可用 IO 中的變更乘數|2|
|尖峰期間所需的 IOPS 總計|800|

若要判斷要準備就緒快取的速率:

- 決定暖快取的最大可接受時間。 這可能是從磁片載入整個資料庫所需花費的時間量, 或在整個資料庫無法以 RAM 載入的情況下, 這是填入 RAM 的最長時間。
- 判斷資料庫的大小, 不包括空白字元。  如需詳細資訊, 請參閱[評估儲存體](#evaluating-for-storage)。  
- 將資料庫大小除以 8 KB;這會是載入資料庫所需的 Io 總數。
- 將總 Io 除以定義的時間範圍內的秒數。

請注意, 計算的速率 (雖然正確) 不會精確, 因為如果 ESE 未設定為具有固定的快取大小, 則先前載入的頁面會被收回, 而 AD DS 預設會使用變數快取大小。

|要收集的資料點|值
|-|-|
|暖的可接受時間上限|10分鐘 (600 秒)
|資料庫大小|2 GB|  
  
|計算步驟|公式|結果|
|-|-|-|
|計算頁面中的資料庫大小|(2 GB &times; 1024 &times; 1024) =*資料庫的大小 (以 KB 為單位*)|2097152 KB|
|計算資料庫中的頁面數目|2097152 kb &divide; 8 kb =*頁面數目*|262144頁面|
|計算完全暖快取所需的 IOPS|262144頁&divide; 600 秒 =*需要 IOPS*|437 IOPS|

## <a name="processing"></a>正在處理

### <a name="evaluating-active-directory-processor-usage"></a>評估 Active Directory 的處理器使用量

在大部分環境中, 如規劃一節中所述適當地調整儲存體、RAM 和網路之後, 管理處理容量的數量將會是值得注意的元件。 評估所需的 CPU 容量有兩個挑戰:

- 環境中的應用程式是否會在共用服務基礎結構中運作良好, 並會在建立更有效率的 Microsoft Active 文章中, 于「追蹤昂貴且無效率的搜尋」一節中討論。啟用目錄的應用程式, 或從下層 SAM 呼叫到 LDAP 呼叫。  
  
  在較大型的環境中, 這很重要的原因是, 編碼不良的應用程式可能會降低 CPU 負載的變動性、「竊取」來自其他應用程式的大量 CPU 時間、人工磁片磁碟機容量需求, 以及不平均的散發負載Dc。  
- 因為 AD DS 是具有各種潛在客戶端的分散式環境, 所以評估「單一用戶端」的代價是因使用模式以及利用 AD DS 的應用程式類型或數量而主觀環保。 簡言之, 非常類似于網路區段, 對於廣泛的適用性而言, 從評估環境中所需總容量的角度來看, 這是比較好的方式。

針對現有的環境, 由於先前已討論過儲存體大小調整, 因此假設儲存體現在已適當調整大小, 因此處理器負載的相關資料是有效的。 再次重申, 確保系統中的瓶頸不是存放裝置的效能, 這是很重要的。 當瓶頸存在且處理器正在等候時, 一旦移除瓶頸, 就會發生閒置狀態。  隨著處理器等候狀態的移除, 根據定義, CPU 使用率會增加, 因為它不再需要等待資料。 因此, 收集效能計數器「邏輯磁片 ( *\<NTDS 資料庫磁片\>磁碟機*) \Avg Disk sec/Read」和「進程 (lsass\\)% Processor Time」。 如果「邏輯磁片 (\\ *\<NTDS 資料庫磁片磁碟機\>* ) \Avg Disk sec/Read」超過10到15毫秒 (這是 Microsoft 支援使用的一般閾值), 「進程 (lsass)% 處理器時間」中的資料會是人為低用於針對儲存體相關的效能問題進行疑難排解。 如前所述, 建議的取樣間隔為15、30或60分鐘。 任何較少的專案通常會太大, 以致于良好的測量。任何較大的東西都會非常順暢地進行每日查看。

### <a name="introduction"></a>簡介

為了規劃網域控制站的容量規劃, 處理能力需要最多的注意與瞭解。 調整系統大小以確保最大效能時, 一律會有一個瓶頸的元件, 而且在適當大小的網域控制站中, 這會是處理器。

類似于網路功能區段, 其中環境的需求會依網站逐一審查, 必須針對所要求的計算容量進行相同的作業。 不同于網路區段, 其中可用的網路技術遠超過一般需求, 請特別注意調整 CPU 容量的大小。  做為任何甚至適中大小的環境;超過一千個並行使用者的任何專案, 都可以對 CPU 造成大量負載。

可惜的是, 由於運用 AD 的用戶端應用程式有極大的變化, 因此每一 CPU 的使用者一般估計會不足抵擋不適用到所有環境。 具體而言, 計算需求會受限於使用者行為和應用程式佈建檔。 因此, 每個環境都必須個別調整大小。

#### <a name="target-site-behavior-profile"></a>目標網站行為設定檔

如先前所述, 在規劃整個網站的容量時, 其目標是以*N* + 1 容量設計為目標, 因此在尖峰期間, 某個系統的失敗會允許在合理的品質層級接續服務。 這表示在「*N*」案例中, 所有方塊的負載都應該小於 100% (較佳但低於 80%)在尖峰期間。

此外, 如果網站中的應用程式和用戶端使用尋找網域控制站的最佳做法 (也就是使用[DsGetDcName](http://msdn.microsoft.com/en-us/library/windows/desktop/ms675983(v=vs.85).aspx)函式), 則用戶端應該相對平均分散, 因為有任何因素數目。

在下一個範例中, 會進行下列假設:

- 網站中的五個 Dc 都有四個 Cpu。
- 工作時間內的目標 CPU 使用量總計是在正常作業條件 ("*n* + 1") 和 60% (否則為 "*n*") 下的 40%。 在非上班時間期間, 目標 CPU 使用量為 80%, 因為備份軟體和其他維護預期會耗用所有可用的資源。

![CPU 使用量圖表](media/capacity-planning-considerations-cpu-chart.png)

針對每個 dc 分析圖表中的資料 (處理器資訊\% (_total) 處理器公用程式):

- 在大部分的情況下, 負載相當平均分佈, 這是用戶端使用 DC 定位器並具有妥善寫入搜尋時所預期的情況。 
- 10% 有數個5分鐘的尖峰, 其中一些最大為 20%。 一般來說, 除非它們導致超過容量計劃目標, 否則調查並不值得。  
- 所有系統的尖峰期間介於大約 8:00 AM 到 9:15 AM 之間。 從大約 5:00 AM 到約 5:00 PM 的順暢轉換, 這通常表示業務週期。 在 5:00 PM 和 4:00 AM 之間的 box 案例中, CPU 使用量的更隨機尖峰會超出容量規劃考慮。

  >[!NOTE]
  >在妥善管理的系統上, 有可能是備份軟體執行中、完整系統防毒程式掃描、硬體或軟體清查、軟體或修補程式部署等等。 因為它們落在尖峰的使用者業務週期之外, 所以不會超過目標。

- 由於每個系統大約是 40%, 而且所有系統都有相同數目的 Cpu, 萬一一個故障或離線, 剩餘的系統會在估計的 53% 執行 (系統 D 的 40% 負載會平均分割並新增至系統 A 和系統 C 的現有 40% 負載)。 基於許多原因, 此線性假設並不完全正確, 但可提供足夠的精確度來測量量測計。  

  **替代案例–** 兩個在 40% 執行的網域控制站:一個網域控制站失敗, 其餘 CPU 的估計 CPU 會是預估的 80%。 這遠超過上述容量計劃的閾值, 同時也開始嚴重限制上述負載設定檔中 10% 到 20% 所見的前端空間量, 這表示尖峰會在「*N*」案例期間將 DC 提升至 90% 到 100%。finitely 會降低回應速度。

### <a name="calculating-cpu-demands"></a>計算 CPU 需求

「進程\\處理器時間百分比」效能物件計數器會加總應用程式的所有線程花費在 CPU 上的時間總計, 並除以已通過的系統時間總量。 這是因為多 CPU 系統上的多執行緒應用程式可能會超過 100% 的 cpu 時間, 而且其轉譯方式與「處理器資訊\\% 處理器公用程式」截然不同。 實際上, 「進程 (lsass)\\% 處理器時間」可視為在 100% 執行以支援處理常式需求所需的 cpu 計數。 值為 200% 時, 表示需要2個 Cpu, 分別是 100%, 以支援完整的 AD DS 負載。 雖然在 100% 容量上執行的 CPU 是最符合成本效益的, 但從 Cpu 和電源和能源耗用量所花費的金錢觀點來看, 有幾個原因, 就是在附錄 A 中詳述, 而在多執行緒系統上更佳未在 100% 執行。

為了配合用戶端負載中的暫時性尖峰, 建議以系統容量介於 40% 和 60% 之間的尖峰期間 CPU 為目標。 使用上述範例時, 這表示 AD DS (lsass 進程) 負載需要 3.33 (60% target) 和 5 (40% target) Cpu。 根據基礎作業系統和其他必要代理程式 (例如防毒軟體、備份、監視等) 的需求, 應新增額外的容量。 雖然代理程式的影響需要依據每個環境進行評估, 但是您可以在單一 CPU 的 5% 和 10% 之間進行估計。 在目前的範例中, 這會建議在尖峰期間內需要 3.43 (60% target) 和 5.1 (40% 目標) Cpu。

若要這麼做, 最簡單的方法是使用 Windows 可靠性和效能監視器 (perfmon) 中的「堆疊區域」視圖, 確定所有計數器都已調整相同。

假設：

- 目標是要盡可能減少伺服器的使用量。 在理想的情況下, 一部伺服器會攜帶負載, 並新增額外的伺服器來進行冗余 (*N* + 1 案例)。

![Lsass 進程的處理器時間圖表 (所有處理器)](media/capacity-planning-considerations-proc-time-chart.png)

從圖表中的資料取得的知識 (進程 (lsass)\\% 處理器時間):

- 工作日會在7:00 開始向上移動, 並于下午5:00 進行減少。
- 尖峰時間最忙碌的期間是從上午9:30 到 11:00 AM。 
  > [!NOTE]
  > 所有效能資料都是歷程記錄。 9:15 的尖峰資料點表示從9:00 到9:15 的負載。
- 7:00 AM 之前會尖峰, 這可能表示從不同的時區或背景基礎結構活動 (例如備份) 載入。 因為尖峰時間 9:30 AM 超過此活動, 所以不相關。
- 網站中有三個網域控制站。

在負載上限時, lsass 會耗用大約 485% 的一個 CPU, 或在 100% 執行4.85 個 Cpu。 根據稍早的數學, 這表示網站需要大約12.25 個 Cpu 的 AD DS。 為背景進程新增上述 5% 到 10% 的建議, 這表示現在取代伺服器會需要大約12.30 到12.35 的 Cpu, 才能支援相同的負載。 現在必須在中考慮成長的環境估計。

### <a name="when-to-tune-ldap-weights"></a>微調 LDAP 權數的時機

在幾種情況下, 應該考慮微調[LdapSrvWeight](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-2000-server/cc957291(v=technet.10)) 。 在容量規劃內容中, 當應用程式或使用者負載未平均平衡, 或基礎系統並未在功能方面平均平衡時, 就會進行這項作業。 除了容量規劃以外的原因, 不在本文的討論範圍內。

微調 LDAP 權數有兩個常見的原因:

- PDC 模擬器是一個範例, 它會影響未平均散發使用者或應用程式載入行為的每個環境。 當某些工具和動作以 PDC 模擬器為目標, 例如群組原則管理工具、第二次嘗試驗證失敗、信任建立等等時, PDC 模擬器上的 CPU 資源可能會比中的其他位置更常要求網站。
  - 如果 CPU 使用率明顯不同, 以降低 PDC 模擬器上的負載, 並增加其他網域控制站的負載, 則只會有更多的負載分佈, 才適合微調此功能。
  - 在此情況下, 請為 PDC 模擬器設定介於50和75之間的 LDAPSrvWeight。
- 網站中具有不同 Cpu 計數的伺服器 (和速度)。  例如, 假設有 2 8 核心伺服器和 1 4 核心伺服器。  最後一個伺服器有一半的處理器, 另外兩部伺服器。  這表示, 妥善發佈的用戶端負載會增加四核心方塊的平均 CPU 負載, 大約八核心的兩倍。
  - 例如, 2 8 核心方塊會在 40% 執行, 而四核心箱會在 80% 執行。
  - 此外, 請考慮在此案例中, [1 8-核心的遺失] 方塊的影響, 特別是四核心的方塊現在會超載。

#### <a name="example-1---pdc"></a>範例 1-PDC

| |使用預設值的使用率|新增 LdapSrvWeight|預估的新使用率|
|-|-|-|-|
|DC 1 (PDC 模擬器)|53%|57|40%|
|DC 2|33%|100|40%|
|DC 3|33%|100|40%|

這裡要注意的是, 如果 PDC 模擬器角色是轉移或收回, 特別是在網站中的另一個網域控制站, 新的 PDC 模擬器會大幅增加。

使用 [[目標網站行為設定檔](#target-site-behavior-profile)] 區段中的範例, 假設網站中的所有三個網域控制站都有四個 cpu。 在正常情況下, 如果其中一個網域控制站有八個 Cpu, 會發生什麼情況？ 會有兩個網域控制站處於 40% 使用率, 另一個為 20% 使用率。 雖然這不是很好的, 但有機會將負載平衡一點。 利用 LDAP 權數來完成此動作。  範例案例如下:

#### <a name="example-2---differing-cpu-counts"></a>範例 2-不同的 CPU 計數

| |處理器資訊\\  %處理器公用程式(_total&nbsp;)<br />使用預設值的使用率|新增 LdapSrvWeight|預估的新使用率|
|-|-|-|-|
|4-CPU DC 1|40|100|大約|
|4-CPU DC 2|40|100|大約|
|8-CPU DC 3|20|200|大約|

不過, 請務必謹慎使用這些案例。 如上面所示, math 看起來很不錯, 而且非常適合紙張。 但在本文中, 規劃「*N* + 1」案例是最重要的一項。 針對每個案例, 都必須計算一個 DC 離線的影響。 在先前的案例中, 負載分配為偶數, 為了確保在「*N*」案例中的 60% 負載, 並在所有伺服器上平均負載平衡, 因此, 當比率保持一致時, 分配就會很好。 查看 PDC 模擬器微調案例, 通常在使用者或應用程式負載不平衡的情況下, 其效果會有很大的差異:

| |微調使用率|新增 LdapSrvWeight|預估的新使用率|
|-|-|-|-|
|DC 1 (PDC 模擬器)|40%|85|47%|
|DC 2|40%|100|53%|
|DC 3|40%|100|53%|

### <a name="virtualization-considerations-for-processing"></a>處理的虛擬化考慮

有兩層的容量規劃需要在虛擬化環境中完成。 在主機層級, 與先前針對網域控制站處理所述的商務週期識別類似, 必須識別尖峰期間的閾值。 因為主機電腦排程 CPU 上的來賓執行緒以取得實體機器上 CPU 的 AD DS 執行緒, 所以建議將基礎主機上的相同目標設為 40% 到 60%。 在下一層, 來賓層, 由於執行緒排程的主體並未變更, 因此來賓中的目標會維持在 40% 到 60% 的範圍內。

在直接對應的案例中, 每一部主機一個來賓, 所有對此點進行的容量規劃, 都必須新增至基礎主機作業系統的需求 (RAM、磁片、網路)。 在共用的主機案例中, 測試表示對基礎處理器的效率有 10% 的影響。 這表示如果網站在目標為 40% 時需要10個 Cpu, 則建議在所有「*N*」來賓配置的虛擬 cpu 數量為11。 在具有混合式實體伺服器和虛擬伺服器分佈的網站中, 修飾詞僅適用于 Vm。 例如, 如果網站有「*N* + 1」案例, 則一個具有10個 cpu 的實體或直接對應伺服器大約相當於一個在主機上具有11個 cpu 的來賓, 並為網域控制站保留11個 cpu。

在分析和計算支援 AD DS 負載所需的 CPU 數量時, 對應至可在實體硬體方面購買的 cpu 數目不一定會完全對應。 虛擬化可讓您不必進位。 虛擬化可減少將計算容量新增至網站所需的工作, 並讓 CPU 能夠加入 VM 的作業更加輕鬆。 它不會讓您不需要正確評估所需的計算能力, 因此當其他 Cpu 需要新增至來賓時, 可以使用基礎硬體。  一如往常, 請記得針對需求的成長進行規劃和監視。

### <a name="calculation-summary-example"></a>計算摘要範例

|系統|尖峰 CPU|
|-|-|-|
|DC 1|120%|
|DC 2|147%|
|Dc 3|218%|
|使用的 CPU 總計|485%|  
  
|目標系統計數|總頻寬 (如上)|
|-|-|
|40% 目標所需的 Cpu|4.85 &divide; . 4 = 12.25|

重複由於此點的重要性, 請*記得規劃成長*。 假設在接下來三年的 50% 成長中, 此環境將需要三年&times;號的 18.375 cpu (12.25 1.5)。 替代方案會在第一年後進行審查, 並視需要增加額外的容量。

### <a name="cross-trust-client-authentication-load-for-ntlm"></a>NTLM 的跨信任用戶端驗證負載

#### <a name="evaluating-cross-trust-client-authentication-load"></a>評估跨信任的用戶端驗證負載

許多環境可能會有一或多個由信任連接的網域。 另一個不使用 Kerberos 驗證之網域中的身分識別要求, 需要使用網域控制站的安全通道, 在目的地網域或路徑中的下一個網域中的另一個網域控制站之間, 進行信任。目的地網域。 使用網域控制站可對受信任網域中的網域控制站進行的並行呼叫數目, 是由稱為**MaxConcurrentAPI**的設定所控制。 針對網域控制站, 確保安全通道可以處理負載量的方法有兩種: 微調**MaxConcurrentAPI** , 或在樹系中建立快捷方式信任。 若要測量跨個別信任的流量量, 請參閱[如何使用 MaxConcurrentApi 設定進行 NTLM 驗證的效能微調](https://support.microsoft.com/kb/2688798)。

在資料收集期間, 在一天的尖峰忙碌期間, 必須收集這項功能, 就像所有其他案例一樣, 資料會很有用。

> [!NOTE]
> 林內和樹林案例可能會導致驗證跨越多個信任, 而每個階段都需要微調。

#### <a name="planning"></a>規劃

根據預設, 有許多應用程式都使用 NTLM 驗證, 或在特定設定案例中使用。 應用程式伺服器的容量和服務增加了許多使用中的用戶端。 另外還有一種趨勢, 那就是用戶端保持會話開啟的時間有限, 而是定期重新連線 (例如電子郵件提取同步)。 高 NTLM 負載的另一個常見範例是需要驗證才能存取網際網路的 Web Proxy 伺服器。

這些應用程式可能會導致大量的 NTLM 驗證負載, 這可能會對 Dc 造成大量壓力, 特別是當使用者和資源位於不同的網域時。

有多種方法可以管理跨信任負載, 在實務上, 這種方式會結合使用, 而不是獨佔的 (或) 案例。 可能的選項包括:

- 藉由找出使用者在使用者所在的相同網域中取用的服務, 減少交互信任用戶端驗證。
- 增加可用的安全通道數目。 這與林內和跨樹系的流量有關, 也稱為快捷方式信任。
- 調整**MaxConcurrentAPI**的預設設定。

若要在現有的伺服器上微調**MaxConcurrentAPI** , 方程式如下:

> *New_MaxConcurrentApi_setting* *(semaphore_acquires* semaphore_time) average_semaphore_hold_time&times; time_collection_length +  &ge; &divide;

如需詳細資訊, [請參閱知識庫文章 2688798:如何使用 MaxConcurrentApi 設定](http://support.microsoft.com/kb/2688798)進行 NTLM 驗證的效能微調。

## <a name="virtualization-considerations"></a>虛擬考量

無, 這是作業系統調整設定。

### <a name="calculation-summary-example"></a>計算摘要範例

|資料類型|值|
|-|-|
|信號取得 (最小)|6161|
|信號取得 (最大值)|6762|
|信號超時|0|
|平均信號保存時間|0.012|
|收集持續時間 (秒)|1:11 分鐘 (71 秒)|
|公式 (從 KB 2688798)|((6762 &ndash; 6161) + 0) &times; 0.012/|
|**MaxConcurrentAPI**的最小值|((6762 &ndash; 6161) + 0) &times; 0.012 &divide; 71 =. 101|

在這段時間內, 此系統會接受預設值。

## <a name="monitoring-for-compliance-with-capacity-planning-goals"></a>監視容量規劃目標的合規性

在本文中, 我們已討論過規劃和調整會轉向使用率目標。 以下是建議閾值的摘要圖表, 必須加以監視, 以確保系統在適當的容量閾值內運作。 請記住, 這些不是效能閾值, 而是容量規劃閾值。 超過這些臨界值的伺服器運作正常, 但卻是開始驗證所有應用程式是否運作良好的時機。 如果說應用程式的行為良好, 就可以開始評估硬體升級或其他設定變更。

|Category|效能計數器|間隔/取樣|目標|警告|
|-|-|-|-|-|
|處理器|處理器資訊 (_total)\\% 處理器公用程式|60分鐘|40%|60%|
|RAM (Windows Server 2008 R2 或更早版本)|Memory\Available MB|< 100 MB|N/A|< 100 MB|
|RAM (Windows Server 2012)|Memory\Long-Term 平均待命快取存留期 (秒)|30分鐘|必須經過測試|必須經過測試|
|Network|Network Interface (\*) \Bytes Sent/sec<br /><br />網路介面 (\*) \Bytes Received/sec|30分鐘|40%|60%|
|儲存體|LogicalDisk ( *\<NTDS 資料庫磁片\>磁碟機*) \Avg Disk sec/Read<br /><br />LogicalDisk ( *\<NTDS 資料庫磁片\>磁碟機*) \Avg Disk sec/Write|60分鐘|10毫秒|15毫秒|
|AD 服務|Netlogon (\*) \Average 信號保存時間|60分鐘|0|1 秒|

## <a name="appendix-a-cpu-sizing-criteria"></a>附錄 A：CPU 大小調整準則

### <a name="definitions"></a>定義

**處理器 (微處理器) –** 讀取和執行程式指示的元件  

**CPU –** 中央處理單位  

**多核心處理器–** 相同整合式線路上的多個 cpu  

**多 cpu –** 多個 cpu, 而不是在相同的整合式線路上  

**邏輯處理器–** 從作業系統的角度來看的一個邏輯運算引擎  

這包括超執行緒、多核心處理器上的一個核心, 或單一核心處理器。  

因為現今的伺服器系統有多個處理器、多個多核心處理器和超執行緒, 所以這項資訊已一般化以涵蓋這兩種案例。 因此, 系統會使用「邏輯處理器」一詞, 因為它代表可用運算引擎的作業系統和應用程式觀點。

### <a name="thread-level-parallelism"></a>執行緒層級平行處理

每個執行緒都是獨立的工作, 因為每個執行緒都有自己的堆疊和指示。 由於 AD DS 是多執行緒處理, 而且可以藉由使用[ntdsutil.exe 在 Active Directory 中查看和設定 LDAP 原則](http://support.microsoft.com/kb/315071)來微調可用的執行緒數目, 因此它會在多個邏輯處理器上進行適當的調整。

### <a name="data-level-parallelism"></a>資料層級平行處理

這牽涉到在一個進程中的多個執行緒之間共用資料 (僅限在 AD DS 程式的情況下), 以及跨多個進程中的多個執行緒 (一般)。 有了過度簡化的情況, 這表示對資料所做的任何變更都會反映到所有執行中的執行緒 (L1、L2、L3) 之所有核心層級中所有執行中的執行緒, 以及更新共用記憶體。 在寫入作業期間, 效能可能會降低, 而在指令處理可以繼續之前, 會使所有不同的記憶體位置保持一致。

### <a name="cpu-speed-vs-multiple-core-considerations"></a>CPU 速度與多核心考慮

一般經驗法則是更快的邏輯處理器會減少處理一連串指令所花費的時間, 而更多的邏輯處理器則表示可以同時執行更多工。 這些經驗法則會在案例變得越來越複雜時, 使用從共用記憶體提取資料、等待資料層級平行處理, 以及管理多個執行緒的額外負荷。 這也是為什麼多核心系統中的擴充性不是線性的原因。

在這些考慮中, 請考慮下列顯而易見: 您的高速公路, 其中每個執行緒都是個別的車、每個通道都是核心, 而速度限制為頻率速度。

1. 如果高速公路上只有一個車輛, 則有兩個車道或12個車道並不重要。 該汽車的速度只會快上一倍, 速度限制就會允許。
1. 假設執行緒所需的資料不會立即可用。 其比喻是一段道路的關閉。 如果高速公路上只有一個車輛, 則速度限制是直到通道重新開啟為止 (從記憶體中提取資料)。
1. 隨著汽車數目的增加, 管理車輛人數的額外負荷也會增加。 比較駕駛的經驗, 以及當道路幾乎空白時所需注意的程度 (例如深夜) 與流量繁重 (例如下午中旬, 但不是尖峰時間) 時的注意事項。 此外, 請考慮在雙通道高速公路上驅動時所需注意的程度, 其中只有一個其他通道需要擔心驅動程式正在執行的作業, 而不是六個通道高速公路, 其中一個必須擔心有許多其他驅動程式正在執行。
   > [!NOTE]
   > 在下一節中, 將會延伸關於尖峰小時案例的比喻:回應時間/系統繁忙程度如何影響效能。

因此, 有關更多或更快處理器的細節會對應用程式行為造成高度主觀, 在 AD DS 的情況下是非常環保, 甚至是在環境內與伺服器之間的差異。 這就是為什麼發行項中稍早的參考不太精確, 而且計算中包含了安全性邊界的原因。 進行預算驅動的購買決策時, 建議您先將 40% (或所需的環境) 的處理器使用優化, 再考慮購買更快的處理器。 在更多處理器上增加的同步處理可減少線性進展中更多處理器的真正&times;優勢 (2 個處理器數目提供少於&times; 2 個可用的額外計算能力)。

> [!NOTE]
> Amdahl 的法律和 Gustafson 法則是相關的概念。

### <a name="response-timehow-the-system-busyness-impacts-performance"></a>回應時間/系統繁忙程度如何影響效能

佇列理論是等候線 (佇列) 的數學研究。 在佇列理論中, 使用量法則是以方程式表示:

*U* k = *B* &divide; *T*

其中, *U* k 是使用率百分比, *B*是忙碌的時間量, 而*T*是觀察到系統的總時間。 轉譯成 Windows 的內容, 這表示處於執行中狀態的100毫微秒 (ns) 間隔執行緒數目除以給定時間間隔內可用的 100-ns 間隔數。 這正是用來計算% 處理器公用程式 (參考[處理器物件](https://docs.microsoft.com/previous-versions/ms804036(v=msdn.10))和[PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/previous-versions/windows/embedded/ms901169(v=msdn.10))) 的公式。

佇列理論也會提供公式:*N* *u k (1* U k), 用來估計以使用率為基礎的等候專案數 (n 是佇列的長度)。  =  &divide; &ndash; 針對所有使用量間隔繪製此圖表, 可提供下列估計值給處理器上的佇列在任何指定的 CPU 負載時間。

![佇列長度](media/capacity-planning-considerations-queue-length.png)

觀察到在 50% CPU 負載之後, 平均總是會在佇列中等候另一個專案, 而且在大約 70% 的 CPU 使用率之後, 會明顯快速增加。

回到本節稍早所使用的駕駛比喻:

- 「下午}」的忙碌時間可能是假設情況下, 落在 40% 到 70% 範圍的某個位置。 有足夠的流量可以讓您挑選任何通道, 而不是 majorly 限制, 而另一部驅動程式的機率很高, 而不需要在旅途中的其他車輛之間「尋找」安全缺口的程度。
- 其中一個會注意到, 當流量接近一小時時, 道路系統會接近 100% 的容量。 變更通道可能會變得非常困難, 因為汽車在一起, 因此必須特別小心。

這就是為什麼在 40% 中, 保守估計容量的長期平均值, 會允許負載的異常尖峰 (例如, 執行數分鐘的編碼查詢不佳) 或一般負載的異常高載 (早上長週末後的第一天)。

上述語句的「% Processor Time」計算與使用法則相同, 因為這是簡化一般讀者的簡單工作。 對於數學上嚴格的:  
- 翻譯[PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/en-us/previous-versions/windows/embedded/ms901169(v=msdn.10))
  - *B* = 100-ns 間隔「閒置」執行緒花費在邏輯處理器上的數目。 [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/en-us/previous-versions/windows/embedded/ms901169(v=msdn.10))計算中 "*X*" 變數的變更
  - *T* = 指定時間範圍內的 100-ns 間隔總數。 [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/en-us/previous-versions/windows/embedded/ms901169(v=msdn.10))計算中 "*Y*" 變數的變更。
  - *U* k = 邏輯處理器的使用率百分比 (依據「閒置執行緒」或% 閒置時間)。  
- 使用數學運算:
  - *U* k = 1 –處理器時間百分比
  - % 處理器時間 = 1 – *U* k
  - % 處理器時間 = 1 – *B*  /  *T*
  - % 處理器時間 = 1 – *X1* – *X0*  /  *Y1* – *Y0*

### <a name="applying-the-concepts-to-capacity-planning"></a>將概念套用至容量規劃

上述數學運算可能會決定系統中所需的邏輯處理器數目似乎回應非常正面複雜。 這就是為什麼調整系統大小的方法, 著重于根據目前的負載來判斷目標使用率上限, 以及計算取得此需求所需的邏輯處理器數目。 此外, 邏輯處理器速度會明顯影響效能、快取效率、記憶體連貫性需求、執行緒排程和同步處理, 以及 imperfectly 平衡的用戶端負載將會對造成重大影響以伺服器為基礎的效能會有所不同。 由於計算能力的成本相對較低, 所以嘗試分析並判斷所需的理想 Cpu 數目, 會比供應商業價值來得多。

40% 不是很難且快速的需求, 這是合理的起點。 Active Directory 的各種取用者都需要各種程度的回應能力。 在某些情況下, 環境可能會以 80% 或 90% 的使用率執行, 做為持續的平均, 因為存取處理器的等候時間將不會明顯影響用戶端效能。 請務必重新逐一查看系統中的多個區域, 其速度比系統中的邏輯處理器慢很多, 包括存取 RAM、存取磁片, 以及透過網路傳送回應。 所有這些專案都必須一起調整。 範例：

- 將更多處理器新增至執行 90% 且磁片系結的系統時, 可能不會大幅提升效能。 較深入的系統分析可能會發現有許多執行緒甚至無法在處理器上取得, 因為它們正在等待 i/o 完成。
- 解決磁片系結問題可能表示先前花很多時間在等候狀態中的執行緒, 將不再處於 i/o 等候狀態, 而且 CPU 時間會有更多競爭, 這表示前一版的 90% 使用率。範例將會移至 100% (因為它不會更高)。 這兩個元件都需要同時調整。
  > [!NOTE]
  > 處理器資訊 (*)\\% 處理器公用程式可能會超過 100% 與具有 "Turbo" 模式的系統。  這是 CPU 在短時間內超過額定處理器速度的位置。  如需更深入的見解, 請參考 CPU 製造商檔和計數器的描述。  

討論整體系統使用考慮也會將對話網域控制站帶入虛擬化的來賓。 [回應時間/系統繁忙程度如何影響效能](#response-timehow-the-system-busyness-impacts-performance)會同時套用至虛擬化案例中的主機和來賓。 這就是為什麼只有一個來賓的主機, 網域控制站 (通常是任何系統) 與實體硬體上的效能幾乎相同。 將其他來賓新增至主機會增加基礎主機的使用率, 因而增加等候時間來取得處理器的存取權, 如先前所述。 簡言之, 邏輯處理器的使用率必須同時在主機和來賓層級進行管理。

若擴充先前的顯而易見, 讓高速公路成為實體硬體, 來賓 VM 將會 analogized 匯流排 (快速匯流排, 直接移至附件所需的目的地)。 假設有下列四個案例:

- 這是休息時間, 在幾乎是空的匯流排上會有附件, 而匯流排也會在幾乎空白的道路上取得。 因為沒有任何流量可以對抗, 所以附件的表現很簡單, 而且會像附件已改成一樣快速地取得。 附件的旅遊時間仍受到速度限制的限制。
- 這是關閉的小時, 因此匯流排幾乎是空的, 但道路上的大部分通路都已關閉, 因此高速公路仍然擁塞。 附件是在擁塞的匯流排上。 雖然附件在匯流排中的位置並沒有很多競爭情況, 但總旅程時間仍是由以外的其餘流量所決定。
- 這是尖峰一小時, 因此高速公路和匯流排會擁塞。 不只是旅程的時間較長, 而且開始和關閉匯流排是一大麻煩, 因為人們會被肩成肩, 而高速公路並不會有更好的效果。 將更多匯流排 (邏輯處理器新增至來賓) 並不表示它們可以更輕鬆地放在道路上, 或者會縮短旅程。
- 最後一種情況是, 雖然它可能會讓您的細節更大, 但匯流排已滿, 但道路並不擁擠。 雖然附件在進入和關閉匯流排時仍有問題, 但在匯流排的道路之後, 旅程也會很有效率。 這是新增更多匯流排 (與來賓的邏輯處理器) 來改善來賓效能的唯一案例。

從這裡來看, 有幾個案例都是在使用 0% 時, 與有 100% 的使用狀態之間, 以及具有不同程度影響的匯流排的 0% 和 100% 使用狀態之間。

將高於 40% CPU 的主體套用為主機和來賓的合理目標, 是合理的啟動, 與上述的相同推理, 也就是佇列的數量。

## <a name="appendix-b-considerations-regarding-different-processor-speeds-and-the-effect-of-processor-power-management-on-processor-speeds"></a>附錄 B：有關不同處理器速度的考慮, 以及處理器電源管理對處理器速度的影響

在處理器選擇的各節中, 假設處理器是在收集資料的整個時間, 以 100% 的頻率速度執行, 而且更換系統將會有相同的速度處理器。 儘管這兩個假設在實務上都是 false, 特別是 Windows Server 2008 R2 和更新版本, 在此情況下, 預設的電源計劃是**平衡**的, 但方法仍然是保守的方法。 雖然可能的錯誤率會增加, 但只有處理器速度增加時, 才會增加安全性的邊界。

- 例如, 在要求 11.25 cpu 的案例中, 如果收集到資料時, 處理器是以半速度執行, 則更精確的估計值可能是 5.125 &divide; 2。
- 不可能保證頻率速度加倍, 會使在指定時段內發生的處理量翻倍。 這是因為處理器花在等待 RAM 或其他系統元件的時間量可能會維持不變。 最後的結果是, 較快的處理器在等候資料提取時, 可能會花費較高的閒置時間百分比。 同樣地, 建議您保持最低的一般分母, 保守, 並避免嘗試計算處理器速度之間的線性比較, 藉以計算可能是 false 的精確度等級。

或者, 如果更換硬體中的處理器速度低於目前的硬體, 則可以安全地增加依比例計算所需的處理器估計。 例如, 它會計算需要10個處理器以承受網站中的負載, 而目前的處理器在 3.3 Ghz 執行, 而更換的處理器會在 2.6 g h z 執行, 而這會降低 21% 的速度。 在此情況下, 12 個處理器會是建議的數量。

話雖如此, 這種變動性並不會變更容量管理處理器使用率目標。 由於處理器頻率速度會根據所要求的負載而動態調整, 因此在較高的負載下執行系統將會產生一個案例, 其中 CPU 以較高的頻率速度來耗費更多時間, 最終目標是在 100% 的 40% 使用率尖峰時的頻率速度狀態。 小於的任何專案都會因為 CPU 速度而產生電力節約, 而在關閉尖峰案例時, 將會進行節流。

> [!NOTE]
> 一個選項是在收集資料時, 關閉處理器上的電源管理 (將電源計劃設定為**高效**能)。 這可讓您更精確地表示目標伺服器上的 CPU 耗用量。

為了調整不同處理器的估計值, 它是安全的, 不包括上面所述的其他系統瓶頸, 而是假設處理器速度加倍倍, 可以執行的處理數量加倍。  目前, 處理器的內部架構在處理器之間的差異很大, 而比較安全的方式來測量使用不同處理器所產生的影響, 而不是利用標準效能評估的 SPECint_rate2006 基準企業.

1. 尋找使用中的處理器 SPECint_rate2006 分數, 以及要使用的方案。
    1. 在標準效能評估公司的網站上, 選取 [**結果**]、[反白顯示**CPU2006**], 然後選取 [**搜尋所有 SPECint_rate2006 結果**]。
    1. 在 [**簡單要求**] 下, 輸入目標處理器的搜尋準則, 例如**處理器符合 e5-2630 (baselinetarget)** , 而**processor 符合 e5-2650 (基準)** 。
    1. 尋找要使用的伺服器和處理器設定 (或關閉的專案 (如果無法使用完全相符的專案), 並記下 [**結果**] 和 [ **# 核心**] 資料行中的值。
1. 若要判斷修飾詞, 請使用下列方程式:
   >((*目標平臺的每個核心分數值*) &times; (基準平臺的每個核心*的 mhz*) &divide; )   &times; ((*目標平臺*的每個核心的 mhz))  

    使用上述範例:
   >(35.83 &times; 2000) &divide; (33.75 &times; 2300) = 0.92
1. 將處理器的估計數目乘以修飾詞。  在上述案例中, 若要從 e5-2650 處理器移至 e5-2630 處理器, 請將計算出&times;的 11.25 cpu 0.92 = 10.35 處理器乘以所需。

## <a name="appendix-c-fundamentals-regarding-the-operating-system-interacting-with-storage"></a>附錄 C：與存放裝置互動之作業系統的相關基本概念

[回應時間/系統繁忙程度影響效能](#response-timehow-the-system-busyness-impacts-performance)中所述的佇列理論概念也適用于儲存體。 您必須熟悉作業系統處理 i/o 的方式, 才能套用這些概念。 在 Microsoft Windows 作業系統中, 會為每個實體磁片建立一個用於保存 i/o 要求的佇列。 不過, 必須對實體磁片進行澄清。 陣列控制器和 San 會以單一實體磁片的形式, 將磁碟陣列的匯總呈現給作業系統。 此外, 陣列控制器和 San 可以將多個磁片匯總成一個陣列集, 然後將這個陣列設為多個「分割區」, 然後再呈現給作業系統做為多個實體磁片 (參考圖)。

![區塊主軸](media/capacity-planning-considerations-block-spindles.png)  

在此圖中, 兩個主軸會鏡像並分割成資料儲存體 (Data 1 和 Data 2) 的邏輯區域。 作業系統會將這些邏輯區域視為個別的實體磁片。

雖然這可能非常令人困惑, 但本附錄中會使用下列術語來識別不同的實體:

- **主軸–** 實際安裝在伺服器中的裝置。
- **Array –** 由控制器匯總的主軸集合。
- **陣列分割–** 匯總陣列的分割
- **LUN –** 用來參考 san 的陣列
- **磁片–** 作業系統會遵循單一實體磁片。
- **磁碟分割–** 作業系統將其視為實體磁片的邏輯分割。

### <a name="operating-system-architecture-considerations"></a>作業系統架構考慮

作業系統會為觀察到的每個磁片建立第一個/先出 (FIFO) i/o 佇列;此磁片可能代表紡錘、陣列或陣列分割區。 從作業系統的觀點來看, 對於處理 i/o 而言, 使用中的佇列愈好。 當 FIFO 佇列已序列化時, 表示發行到儲存子系統的所有 i/o 都必須按照要求抵達的順序來處理。 藉由將作業系統所觀察到的每個磁片與主軸/陣列相互關聯, 作業系統現在會針對每一組唯一的磁片維護一個 i/o 佇列, 藉此消除對磁片上的不足 i/o 資源的爭用, 並將 i/o 需求隔離到單一硬碟. 例外狀況是 Windows Server 2008 引進 i/o 優先順序的概念, 而設計使用「低」優先順序的應用程式則是以這種正常循序執行, 並帶回一步。 未特別編碼的應用程式會利用「低」優先順序預設為「正常」。

### <a name="introducing-simple-storage-subsystems"></a>簡單的儲存子系統簡介

從簡單的範例 (電腦內的單一硬碟) 開始, 將會提供一個元件的分析。 將此細分成主要的儲存子系統元件, 系統包含:

- **1 –** 10000 RPM ULTRA FAST scsi HD (ULTRA 快速 scsi 具有 20 MB/s 傳輸速率)
- **1 –** SCSI 匯流排 (纜線)
- **1 –** Ultra 快速 SCSI 介面卡
- **1 –** 32 位 33 MHz PCI 匯流排

一旦識別出元件, 就可以計算出有多少資料可以傳輸系統, 或可以處理多少 i/o。 請注意, 可以傳輸系統的 i/o 數量和資料數量是相互關聯的, 但不是相同的。 此相互關聯取決於磁片 i/o 是隨機或連續, 還是區塊大小。 (所有資料都會以區塊的形式寫入至磁片, 但使用不同區塊大小的應用程式則會不同)。以元件為基礎:

- **硬碟–** 平均 10000-RPM 硬碟有7毫秒 (ms) 的搜尋時間和3毫秒的存取時間。 「搜尋時間」是讀取/寫入標頭移至盤上某個位置所需的平均時間量。 存取時間是將資料讀取或寫入磁片的平均時間量, 一旦標頭位於正確的位置即可。 因此, 在 10000-RPM HD 中讀取唯一資料區塊的平均時間, 會形成搜尋和存取, 總計每個資料區塊大約10毫秒 (或 .010 秒)。

  當每個磁片存取需要將標頭移至磁片上的新位置時, 讀取/寫入行為稱為「隨機」。 因此, 當所有 i/o 都是隨機的時, 10000-RPM HD 可以處理大約每秒 100 i/o (IOPS) (每秒的公式為1000毫秒, 每個 i/o 為10毫秒, 或 1000/10 = 100 IOPS)。

  或者, 當所有 i/o 發生于 HD 上的相鄰磁區時, 這稱為「連續 i/o」。 連續 i/o 沒有搜尋時間, 因為當第一個 i/o 完成時, 讀取/寫入標頭會在資料的下一個區塊儲存在 HD 上的開始位置。 因此, 10000-RPM HD 能夠處理每秒大約333個 i/o (每秒1000毫秒除以每個 i/o 3 毫秒)。

  >[!NOTE]
  >這個範例不會反映磁碟快取, 其中一個圓柱的資料通常會保留下來。 在此情況下, 第一個 i/o 需要10毫秒, 而磁片則會讀取整個圓柱。 從快取中滿足所有其他的順序 i/o。 因此, 磁碟快取可能會改善順序 i/o 效能。
  
  到目前為止, 硬碟的傳輸速率並不相關。 無論硬碟為 20 MB/s Ultra 或 Ultra3 160 MB/秒, 10000-RPM HD 可以處理的實際 IOPS 量是 ~ 100 隨機或 ~ 300 順序 i/o。 當區塊大小根據寫入磁片磁碟機的應用程式而變更時, 每個 i/o 提取的資料量會不同。 例如, 如果區塊大小為 8 KB, 則 100 i/o 作業會讀取或寫入硬碟 (總共 800 KB)。 不過, 如果區塊大小為 32 KB, 則 100 i/o 會讀取/寫入至硬碟的 3200 KB (3.2 MB)。 只要 SCSI 傳輸速率超過傳輸的資料總量, 取得「更快」的傳輸速率磁片磁碟機就不會得到任何東西。 如需比較, 請參閱下表。

  | |7200 RPM 9ms 搜尋, 4 毫秒存取|10000 RPM 7 毫秒搜尋, 3ms 存取|15000 RPM 4 毫秒搜尋, 2 毫秒存取
  |-|-|-|-|
  |隨機 i/o|80|100|150|
  |連續 i/o|250|300|500|  
  
  |10000 RPM 磁片磁碟機|8 KB 區塊大小 (Active Directory Jet)|
  |-|-|
  |隨機 i/o|800 KB/秒|
  |連續 i/o|2400 KB/秒|

- **SCSI 背板 (匯流排) –** 瞭解「SCSI 背板 (匯流排)」或在此案例中, 帶狀線會影響儲存子系統的輸送量, 取決於區塊大小的知識。 基本上, 問題是, 如果 i/o 是在 8 KB 區塊中, 匯流排會處理多少 i/o？ 在此案例中, SCSI 匯流排為 20 MB/s, 或 20480 KB/s。 20480 KB/s 除以 8 KB 區塊, 最多會產生 SCSI 匯流排所支援的大約 2500 IOPS。

  >[!NOTE]
  >下表中的圖形代表一個範例。 大部分連結的存放裝置目前使用 PCI Express, 以提供更高的輸送量。  
  
  |SCSI 匯流排每個區塊大小支援的 i/o|2 KB 區塊大小|8 KB 區塊大小 (AD Jet) (SQL Server 7.0/SQL Server 2000)
  |-|-|-|
  |20 MB/秒|10000|2500|
  |40 MB/秒|20000|5000|
  |128 MB/秒|65,536|16384|
  |320 MB/秒|160000|40000|

  根據此圖表的判斷, 在呈現的案例中, 不論使用何種情況, 匯流排都不會是瓶頸, 因為主軸最大值為 100 i/o, 低於上述任何閾值。

  >[!NOTE]
  >這會假設 SCSI 匯流排的 100% 有效率。
  
- **SCSI 介面卡–** 若要判斷此可處理的 i/o 數量, 必須檢查製造商的規格。 將 i/o 要求導向適當的裝置需要處理某種排序, 因此可以處理的 i/o 數量取決於 SCSI 介面卡 (或陣列控制器) 處理器。

  在此範例中, 將會假設 1000 i/o 可以進行處理。

- **PCI 匯流排–** 這是一個經常被忽略的元件。 在此範例中, 這不會是瓶頸;不過, 隨著系統相應增加, 它可能會成為瓶頸。 如需參考, 32 位 PCI 匯流排在33Mhz 運作, 可在理論上傳輸 133 MB/秒的資料。 以下是方程式:  
  > 32位&divide;每個位元組&times; 8 位 33 MHz = 133 MB/s。  

  請注意, 是理論上的限制;實際上, 只有大約 50% 的最大值才達到, 雖然在某些高載案例中, 您可以在短時間內取得 75% 的效率。

  66Mhz 64 位 PCI 匯流排可支援理論上限 (64 位&divide;每個位元組&times;的8位 66 Mhz) = 528 MB/秒。此外, 其他任何裝置 (例如網路介面卡、第二個 SCSI 控制器等等) 都會減少可用的頻寬當頻寬是共用的, 且裝置會爭用有限的資源。

分析此儲存子系統的元件之後, 主軸就是可要求的 i/o 數量的限制因素, 因而是可以傳輸系統的資料量。 具體而言, 在 AD DS 案例中, 這是每秒100個隨機 i/o (以 8 KB 遞增), 在存取 Jet 資料庫時, 總計為每秒 800 KB。 或者, 專門配置給記錄檔的磁鍼的最大輸送量會受到下列限制:300每秒的連續 i/o (8 KB 增量), 總計為每秒 2400 KB (2.4 MB)。

現在, 分析了簡單的設定, 下表將示範當儲存子系統中的元件變更或新增時, 瓶頸將會發生的位置。

|注意|瓶頸分析|Disk|Bus|介面卡|PCI 匯流排|
|-|-|-|-|-|-|
|這是新增第二個磁片後的網域控制站設定。 磁片設定代表 800 KB/s 的瓶頸。|新增1個磁片 (總計 = 2)<br /><br />I/o 是隨機的<br /><br />4 KB 區塊大小<br /><br />10000 RPM HD|200 i/o 總計<br />800 KB/s 總計。| | | |
|新增7個磁片之後, 磁片設定仍然代表 3200 KB/s 的瓶頸。|**新增7個磁片 (總計 = 8)**  <br /><br />I/o 是隨機的<br /><br />4 KB 區塊大小<br /><br />10000 RPM HD|800 i/o 總計。<br />3200 KB/s 總計| | | |
|將 i/o 變更為連續之後, 網路介面卡會成為瓶頸, 因為它受限於 1000 IOPS。|新增7個磁片 (總計 = 8)<br /><br />**I/o 是連續的**<br /><br />4 KB 區塊大小<br /><br />10000 RPM HD| | |2400 i/o sec 可以讀取/寫入磁片, 控制器限制為 1000 IOPS| |
|將網路介面卡更換為支援 10000 IOPS 的 SCSI 介面卡之後, 瓶頸會回到磁片設定。|新增7個磁片 (總計 = 8)<br /><br />I/o 是隨機的<br /><br />4 KB 區塊大小<br /><br />10000 RPM HD<br /><br />**升級 SCSI 介面卡 (現在支援 10000 i/o)**|800 i/o 總計。<br />3200 KB/s 總計| | | |
|將區塊大小增加到 32 KB 之後, 匯流排就會成為瓶頸, 因為它只支援 20 MB/s。|新增7個磁片 (總計 = 8)<br /><br />I/o 是隨機的<br /><br />**32 KB 區塊大小**<br /><br />10000 RPM HD| |800 i/o 總計。 25600 KB/s (25 MB/s) 可以讀取/寫入磁片。<br /><br />匯流排僅支援 20 MB/秒| | |
|升級匯流排並新增更多磁片之後, 磁片會維持瓶頸。|**新增13個磁片 (總計 = 14)**<br /><br />新增具有14個磁片的第二個 SCSI 介面卡<br /><br />I/o 是隨機的<br /><br />4 KB 區塊大小<br /><br />10000 RPM HD<br /><br />**升級至 320 MB/s SCSI 匯流排**|2800 i/o<br /><br />11200 KB/秒 (10.9 MB/s)| | | |
|將 i/o 變更為連續之後, 磁片會維持瓶頸。|新增13個磁片 (總計 = 14)<br /><br />新增具有14個磁片的第二個 SCSI 介面卡<br /><br />**I/o 是連續的**<br /><br />4 KB 區塊大小<br /><br />10000 RPM HD<br /><br />升級至 320 MB/s SCSI 匯流排|8400 i/o<br /><br />33600 KB\s<br /><br />(32.8 MB\s)| | | |
|新增更快的硬碟之後, 磁片會維持瓶頸。|新增13個磁片 (總計 = 14)<br /><br />新增具有14個磁片的第二個 SCSI 介面卡<br /><br />I/o 是連續的<br /><br />4 KB 區塊大小<br /><br />**15000 RPM HD**<br /><br />升級至 320 MB/s SCSI 匯流排|14000 i/o<br /><br />56000 KB/秒<br /><br />(54.7 MB/秒)| | | |
|將區塊大小增加到 32 KB 之後, PCI 匯流排就會變成瓶頸。|新增13個磁片 (總計 = 14)<br /><br />新增具有14個磁片的第二個 SCSI 介面卡<br /><br />I/o 是連續的<br /><br />**32 KB 區塊大小**<br /><br />15000 RPM HD<br /><br />升級至 320 MB/s SCSI 匯流排| | | |14000 i/o<br /><br />448000 KB/秒<br /><br />(437 MB/s) 是磁片軸的讀取/寫入限制。<br /><br />PCI 匯流排支援的理論最大值為 133 MB/秒 (最好是 75% 的效率)。|

### <a name="introducing-raid"></a>RAID 簡介

引進陣列控制器時, 儲存子系統的本質並不會大幅變更;它只會取代計算中的 SCSI 介面卡。 當使用各種陣列層級 (例如 RAID 0、RAID 1 或 RAID 5) 時, 「變更」是讀取和寫入資料到磁片的成本。

在 RAID 0 中, 資料會分割到 RAID 集中的所有磁片上。 這表示在讀取或寫入作業期間, 會從每個磁片提取或推送資料的一部分, 增加在同一時間週期內傳輸系統的資料量。 因此, 在一秒內, 每個主軸 (又假設 10000-RPM 磁片磁碟機) 都可以執行 100 i/o 作業。 可支援的 i/o 總數為每個主軸每秒100個 i/o 的時間量 (每秒產生 100 * N 個 i/o)。

![邏輯 d: 磁片磁碟機](media/capacity-planning-considerations-logical-d-drive.png)

在 RAID 1 中, 資料會在一對主軸間進行鏡像 (重複) 以進行冗余。 因此, 在執行讀取 i/o 作業時, 可以從集合中的兩個主軸讀取資料。 這可有效地從兩個磁片取得 i/o 容量, 以供讀取作業期間使用。 有一點要注意的是, 寫入作業在 RAID 1 中沒有任何效能優勢。 這是因為需要將相同的資料寫入兩個磁片磁碟機, 以因應重複的目的。 雖然它不會花更長的時間, 因為資料的寫入會同時在這兩個主軸上發生, 因為這兩個磁鍼都被佔用重複的資料, 所以寫入 i/o 作業基本上會防止兩次讀取作業發生。 因此, 每個寫入 i/o 都會成本兩個讀取 i/o。 您可以從該資訊建立公式, 以判斷目前發生的 i/o 作業總數:  

> *讀取 i/o* &times; + 2*寫入 i/o*  = 已*耗用的可用磁片 i/o 總計*  

當讀取至寫入的比率和已知道的磁鍼數目時, 可以從上述方程式衍生下列方程式, 以識別陣列可以支援的最大 i/o:  

> *每個主軸的最大 IOPS*&times;  +  &divide; &times;2 個主軸 [(% 讀取% 寫入) (% 讀取 + 2% 寫入)] = IOPS 總計&times;

RAID 1 + 0 的行為與 RAID 1 完全相同, 關於讀取和寫入的費用。 不過, i/o 現在會在每個鏡像集合中等量。 如果  

> *每個主軸的最大 IOPS*&times;  +  &divide; &times;2 個主軸 [(% 讀取% 寫入) (% 讀取 + 2% 寫入)] = Total i/o &times;  

在 raid 1 集合中, 當 raid 1 集的多重性 (*N*) 為等量時, 可以處理的總 i/o 會每個 RAID 1 &times;設定為 N 個 i/o:  

> *N* &times; &times; &divide;  +  &times;{每個主軸2主軸的最大 IOPS [(% 讀取% 寫入) (% 讀取 + 2% 寫入)]} = &times; *IOPS 總計*

在 RAID 5 中, 有時稱為*n* + 1 RAID, 資料會在*n*個主軸上等量, 而同位資訊則會寫入 "+ 1" 主軸。 不過, 在執行寫入 i/o 時, RAID 5 比 RAID 1 或 1 + 0 高得多。 每次將寫入 i/o 提交至陣列時, RAID 5 都會執行下列進程:

1. 讀取舊資料
1. 讀取舊的同位
1. 寫入新的資料
1. 撰寫新的同位

因為作業系統提交給陣列控制器的每個寫入 i/o 要求, 都需要完成四個 i/o 作業, 所以提交的寫入要求需要四倍的時間才能完成為單一讀取 i/o。 若要衍生公式以從作業系統觀點將 i/o 要求轉譯成主軸所遇到的要求:  

> *讀取 i/o* &times; + 4*寫入 i/o*  = *總計*i/o  

同樣地, 在 RAID 1 集合中, 當讀取至寫入的比率和磁鍼數目已知時, 可以從上述方程式衍生出下列方程式, 以識別陣列可支援的最大 i/o (請注意, 主軸的總數目不包括e 「磁片磁碟機」遺失同位檢查):  

> *每個主軸的 IOPS* +  &divide; &times; &times; (主軸– 1) [(% 讀取% 寫入) (% 讀取 + 4% 寫入)] = IOPS 總計&times;

### <a name="introducing-sans"></a>San 簡介

擴大儲存子系統的複雜性, 當 SAN 引進環境時, 所述的基本原則並不會改變, 不過, 所有連線到 SAN 的系統必須將 i/o 行為納入考慮。 因為使用 SAN 的其中一個主要優點是在內部或外部連接的儲存體上有額外的冗余量, 所以容量規劃現在需要考慮容錯需求。 此外, 引進了更多需要評估的元件。 將 SAN 分解成元件部分:

- SCSI 或光纖通道硬碟
- 儲存體單位通道背板
- 儲存單位
- 存放控制器模組
- SAN 交換器 (es)
- HBA
- PCI 匯流排

設計任何系統的冗余時, 會包含額外的元件以容納可能發生失敗的情況。 在容量規劃時, 從可用的資源中排除多餘的元件是非常重要的。 例如, 如果 SAN 有兩個控制器模組, 一個控制器模組的 i/o 容量就是應該用於系統可用的總 i/o 輸送量。 這是因為如果某個控制器失敗, 則所有連線系統所要求的整個 i/o 負載, 都必須由剩餘的控制器處理。 由於所有容量規劃都是針對尖峰使用期間進行, 因此不應該將多餘的元件納入可用的資源中, 而且規劃的尖峰使用率不應超過系統的 80% 飽和度 (以配合高載或異常系統)行為)。 同樣地, 多餘的 SAN 交換器、儲存單元和主軸不應納入 i/o 計算中。

分析 SCSI 或光纖通道硬碟的行為時, 如先前所述分析行為的方法並不會變更。 雖然每個通訊協定都有一些優點和缺點, 但每個磁片的限制因素是硬碟的機械限制。

分析儲存裝置上的通道, 與計算 SCSI 匯流排上可用的資源, 或頻寬 (例如 20 MB/s) 除以區塊大小 (例如 8 KB) 完全相同。 這偏離了簡單的上一個範例, 就是在多個通道的匯總中。 例如, 如果有6個通道, 每個都支援 20 MB/秒的最大傳輸速率, 則可用的 i/o 和資料傳輸總量為 100 MB/秒 (這是正確的, 不是 120 MB/s)。 同樣地, 容錯是此計算中的主要播放程式, 在整個通道遺失的情況下, 系統只會保留5個運作中的通道。 因此, 為了確保在發生失敗時繼續符合效能預期, 所有儲存通道的總輸送量不應超過 100 MB/秒 (這會假設負載和容錯會平均分散到所有通道)。 將此轉換成 i/o 設定檔取決於應用程式的行為。 在 Active Directory Jet i/o 的情況下, 這會與大約每秒12500個 i/o (每個 i/o 的 100 MB/s &divide; 8 KB) 相互關聯。

接下來, 需要取得控制器模組的製造商規格, 才能瞭解每個模組可支援的輸送量。 在此範例中, SAN 有兩個控制器模組, 分別支援 7500 i/o。 如果不需要冗余, 系統的總輸送量可能是 15000 IOPS。 在計算失敗情況下的最大輸送量時, 其限制為一個控制器的輸送量, 或 7500 IOPS。 此臨界值低於 12500 IOPS (假設為 4 KB 區塊大小), 所有儲存通道皆可支援的最大值, 因此目前為分析中的瓶頸。 仍在進行規劃時, 所需規劃的最大 i/o 會是 10400 i/o。

當資料結束控制器模組時, 它會傳輸以 1 GB/s (或每秒 1 Gigabit) 分級的光纖通道連接。 若要將此與其他計量相互關聯, 1 gb/s 會變成 128 MB/s (1 gb &divide; /s 8 位/位元組)。 因為這超出了儲存單位 (100 MB/s) 中所有通道的總頻寬, 這不會造成系統瓶頸。 此外, 因為這只是兩個通道中的其中一個 (額外的 1 GB/s 光纖通道連線為冗余), 如果一個連線失敗, 則剩餘的連線仍會有足夠的容量來處理所有要求的資料傳輸。

路由傳送至伺服器時, 資料最可能會傳輸 SAN 交換器。 當 SAN 交換器必須處理傳入的 i/o 要求並將其轉送到適當的埠時, 交換器會限制可處理的 i/o 數量, 不過, 製造商規格將需要判斷該限制為何。 例如, 如果有兩個參數, 且每個參數都可以處理 10000 IOPS, 則輸送量總計將會是 20000 IOPS。 同樣地, 容錯功能是一項考慮, 如果一個交換器失敗, 系統的總輸送量會是 10000 IOPS。 因為在正常作業中不會超過 80% 使用率, 所以使用不超過8000的 i/o 應該是目標。

最後, 安裝在伺服器中的 HBA 也會限制可處理的 i/o 數量。 通常會針對冗余安裝第二個 HBA, 但就像 SAN 交換器一樣, 計算可處理的最大 i/o 時, *N* &ndash; 1 個 hba 的總輸送量就是系統的最大擴充性。

### <a name="caching-considerations"></a>快取考慮

快取是其中一個元件, 可能會大幅影響儲存系統中任何時間點的整體效能。 有關快取演算法的詳細分析已超出本文的範圍;不過, 關於在磁片子系統上快取的一些基本聲明是值得注意的:

- 快取會改善持續的連續寫入 i/o, 因為它可以將許多較小的寫入作業緩衝處理至較大的 i/o 區塊, 並以較少但較大的區塊大小將其移到儲存體中。 這會減少隨機 i/o 總計和連續 i/o 總計, 因此可為其他 i/o 提供更多資源可用性。
- 快取不會改善儲存子系統的持續寫入 i/o 輸送量。 它只允許緩衝寫入, 直到可用來認可資料的磁鍼為止。 當儲存子系統中的磁片區所有可用 i/o 都長時間飽和時, 快取最後就會填滿。 若要清空快取, 必須配置高載或額外磁鍼之間的足夠時間, 才能提供足夠的 i/o 以允許快取排清。

  較大的快取只允許緩衝處理更多資料。 這表示可以容納較長的飽和時間。

  在正常的操作儲存子系統中, 作業系統將會體驗改善的寫入效能, 因為資料只需要寫入快取。 當基礎媒體以 i/o 飽和之後, 快取的填滿和寫入效能將會恢復到磁片速度。

- 當快取讀取 i/o 時, 快取最有利的案例是將資料依序儲存在磁片上, 而且快取可以預先讀取 (這會假設下一個磁區包含接下來會要求的資料)。
- 當讀取 i/o 是隨機的時, 磁片磁碟機控制器的快取不太可能會對可從磁片讀取的資料量提供任何增強功能。 如果作業系統或以應用程式為基礎的快取大小大於以硬體為基礎的快取大小, 則任何增強功能都不存在。

  在 Active Directory 的情況下, 快取只受限於 RAM 數量。

### <a name="ssd-considerations"></a>SSD 考慮

Ssd 是與主軸型硬碟完全不同的動物。 但這兩個主要的準則仍然存在:「它可以處理多少 IOPS？」 和「這些 IOPS 的延遲為何？」 相較于以主軸為基礎的硬碟, Ssd 可以處理更大量的 i/o, 而且可能會有較低的延遲。 一般來說, 在撰寫本文時, 雖然 Ssd 在每個 Gb 的成本上都是昂貴的, 但它們在儲存體效能方面的成本會非常低。

考量：

- 對製造商設計而言, IOPS 和延遲都非常主觀, 而在某些情況下, 觀察到比主軸型技術更差的執行。 簡單地說, 依磁片磁碟機審查和驗證製造商規格磁片磁碟機, 並不會假設任何 generalities, 是比較重要的。
- IOPS 類型可能會有非常不同的數位, 視其是否為讀取或寫入而定。 一般來說, AD DS 服務主要是以讀取為主, 而不會受到其他應用程式案例的影響。
- 「寫入耐用」–這是 SSD 資料格最後會磨損的概念。各種製造商會處理這項挑戰不同的 fashions。 至少針對資料庫磁片磁碟機, 主要的讀取 i/o 設定檔可讓您 downplaying 這項考慮的重要性, 因為資料並不是高度變動。

### <a name="summary"></a>總結

考慮儲存的其中一種方式是 picturing 的家用管道。 想像資料儲存所在之媒體的 IOPS 是家用的主要清空。 當這種情況變長 (例如, 在管道中的根) 或有限制 (它是折迭或太小) 時, 家庭中的所有接收器都會在使用太多水 (過多來賓) 時備份。 這完全類似于共用環境, 其中一或多個系統會利用具有相同基礎媒體的 SAN/NAS/iSCSI 上的共用存放裝置。 可以採取不同的方法來解決不同的情況:

- 折迭或大小過小的清空需要完整的更換和修正。 這類似于新增硬體, 或使用整個基礎結構中的共用存放裝置來轉散發系統。
- 「已堵塞」管道通常表示識別一個或多個違規問題, 並移除這些問題。 在存放裝置案例中, 這可能是儲存體或系統層級的備份、跨所有伺服器同步處理防毒軟體掃描, 以及在尖峰期間執行的同步處理磁碟重組軟體。

在任何配管設計中, 會將多個耗盡摘要放入主清空。 如果任何專案停止了其中一個耗盡或連接點, 則只會備份該連接點背後的東西。 在儲存案例中, 這可能是多載交換器 (SAN/NAS/iSCSI 案例)、驅動程式相容性問題 (錯誤的驅動程式/HBA 固件/storport 組合), 或備份/防毒軟體/磁碟重組。 若要判斷存放裝置「管道」是否夠大, 必須測量 IOPS 和 i/o 大小。 每個聯合都會將它們相加, 以確保有足夠的「管道直徑」。

## <a name="appendix-d---discussion-on-storage-troubleshooting---environments-where-providing-at-least-as-much-ram-as-the-database-size-is-not-a-viable-option"></a>附錄 D-討論儲存體疑難排解-提供至少 RAM 作為資料庫大小的環境不是可行的選項

瞭解這些建議存在的原因會很有説明, 讓您可以容納儲存體技術的變更。 這些建議有兩個原因。 第一種是 IO 的隔離, 因此作業系統主軸上的效能問題 (也就是分頁) 並不會影響資料庫和 i/o 設定檔的效能。 第二個是 AD DS 的記錄檔 (和大部分的資料庫) 本質上是連續的, 而且以主軸為基礎的硬碟和快取在與序列 i/o 搭配使用時, 會有相當大的效能優勢, 相較于作業系統的更隨機 i/o 模式純粹是 AD DS 資料庫磁片磁碟機的隨機 i/o 模式。 藉由將連續 i/o 隔離到不同的實體磁片磁碟機, 可以增加輸送量。 現今儲存選項的挑戰在於, 這些建議背後的基本假設已不再成立。 在許多虛擬化的儲存案例中, 例如 iSCSI、SAN、NAS 和虛擬磁片影像檔, 基礎儲存體媒體會在多部主機之間共用, 因此可完全否定「隔離 IO」和「連續 i/o 優化」方面。 事實上, 這些案例會增加額外的複雜性層級, 而存取共用媒體的其他主機可能會降低網域控制站的回應速度。

在規劃存放裝置效能中, 有三個要考慮的類別: 冷快取狀態、準備就緒快取狀態, 以及備份/還原。 在一開始重新開機網域控制站或重新開機 Active Directory 服務, 且 RAM 中沒有任何 Active Directory 資料的情況下, 會發生冷快取狀態。 暖快取狀態是網域控制站處於穩定狀態且資料庫已快取的位置。 請務必注意, 因為它們會驅動非常不同的效能設定檔, 而且有足夠的 RAM 來快取整個資料庫, 在快取很冷時並不會協助效能。 您可以考慮使用下列兩種案例的效能設計: 以下的比喻, 預熱冷快取是「短期衝刺」, 而執行具有暖快取的伺服器是「marathon」。

對於冷快取和暖快取案例, 問題會變成存放裝置可將資料從磁片移至記憶體的速度。 準備快取是一種案例, 在一段時間後, 效能會隨著更多查詢重複使用資料、快取點擊率增加, 以及需要移至磁片的頻率減少而提升。 如此一來, 對磁片的效能影響也會降低。 在等候快取暖並成長到最大的系統相依允許大小的情況下, 效能降低只是暫時性的。 您可以簡化交談, 以瞭解資料可以從磁片取出的速度, 而且是可供 Active Directory 的簡單 IOPS 量, 這對可從基礎儲存體取得的 IOPS 而言是主觀的。 從規劃的觀點來看, 由於預先快取和備份/還原案例是以例外的方式進行, 因此通常會在幾小時內發生, 而且對於 DC 的負載是主觀的, 一般建議並不存在, 除非這些活動已排程適用于非尖峰時間。

在大部分情況下, AD DS 主要是讀取 IO, 通常是 90% 讀取/10% 寫入的比率。 讀取 i/o 通常是使用者體驗的瓶頸, 而在寫入 IO 中, 會導致寫入效能降低。 由於 ntds.dit 的 i/o 主要是隨機的, 因此快取通常會為讀取 IO 提供最小的優點, 因此更重要的是要正確設定讀取 i/o 設定檔的儲存體。

針對正常作業狀況, 儲存體規劃目標會將要求的等候時間最小化, 從磁片傳回 AD DS。 這基本上表示未處理和擱置 i/o 的數目小於或等於磁片的路徑數目。 有多種方法可以測量這種情況。 在效能監視案例中, 一般建議是 LogicalDisk ( *\<NTDS 資料庫磁片\>磁碟機*) \Avg Disk sec/Read 少於20毫秒。 所需的操作閾值必須較低, 最好是盡可能接近儲存體的速度, 在2到6毫秒 (.006 second) 範圍內, 視儲存體類型而定。

範例：

![儲存體延遲圖表](media/capacity-planning-considerations-storage-latency.png)

分析圖表:

- **左邊的綠色橢圓形–** 延遲會在10毫秒後保持一致。 負載會從 800 IOPS 增加到 2400 IOPS。 這是基礎儲存體處理 i/o 要求的速度的絕對樓層。 這會受限於儲存體解決方案的細節。
- **右側的 Burgundy 橢圓形–** 輸送量會從綠色圓形結束到資料收集的結尾, 而延遲會持續增加。 這表示當要求磁片區超過基礎儲存體的實體限制時, 要求花費在佇列中等候送出至儲存子系統的時間愈久。

應用此知識:

- 對**使用者查詢大型群組成員資格的影響-** 假設這需要從磁片讀取 1 MB 的資料、i/o 的數量及所需的時間, 如下所示:
  - Active Directory 資料庫頁面的大小為 8 KB。 
  - 至少需要從磁片讀取128個頁面。 
  - 假設沒有快取任何內容, 在樓層 (10 毫秒), 這將會花費最短1.28 秒的時間從磁片載入資料, 以將它傳回給用戶端。 在20毫秒, 儲存體上的輸送量在超量而時有很長的時間, 而且也是建議的最大值, 需要2.5 秒的時間從磁片取得資料, 才能將它傳回給使用者。  
- **準備就緒快取的速率為何–** 假設用戶端負載即將達到此儲存體範例的輸送量上限, 快取將會以每個 IO 2400 IOPS &times; 8 KB 的速度來預熱。 或者, 大約每秒 20 MB/s, 每隔53秒會載入大約 1 GB 的資料庫到 RAM。

> [!NOTE]
> 當元件積極地讀取或寫入磁片時 (例如, 當系統正在進行備份時, 或 AD DS 執行垃圾收集時), 短時間內觀察延遲的速度會很正常。 應該提供計算頂端的額外標頭空間, 以配合這些定期事件。 目標是要提供足夠的輸送量來容納這些案例, 而不會影響正常的功能。

如您所見, 根據存放裝置的設計, 快取可能暖的速度會有一個實體限制。 快取的暖是傳入的用戶端要求, 最高可達基礎儲存體所能提供的速率。 在尖峰時間執行腳本以「預先準備」快取, 將可讓您以實際用戶端要求的方式來進行負載的競爭。 這可能會對傳遞用戶端所需的資料造成負面影響, 因為根據設計, 它會產生不足的磁片資源競賽, 因為人工嘗試暖快取會載入與連接 DC 的用戶端無關的資料。
